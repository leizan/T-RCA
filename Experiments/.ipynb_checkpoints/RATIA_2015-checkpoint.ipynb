{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8bd24b5b-b65c-4a9f-8671-23be66cd1675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from numpy.linalg import inv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab541366-1925-4939-83fe-cb1d366adfed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RAITIA2011:\n",
    "    def __init__(self, data, categorical_nodes, gamma_max, sig_level, threshold_for_discretization_dict):\n",
    "        self.graph = nx.DiGraph()\n",
    "        # todo: verify categorical data\n",
    "        # categorical_nodes = list(data.columns)\n",
    "        # for tnode in data.columns:\n",
    "        #     temp_var = []\n",
    "        #     for v in data[tnode]:\n",
    "        #         if v > threshold_for_discretization_dict[tnode][0]:\n",
    "        #             temp_var.append(True)\n",
    "        #         else:\n",
    "        #             temp_var.append(False)\n",
    "        #     print(tnode, sum(temp_var))\n",
    "        #     data[tnode] = temp_var\n",
    "        #     # threshold_for_discretization_dict[tnode] = []\n",
    "        # print(data)\n",
    "\n",
    "\n",
    "        self.qualitative_nodes = categorical_nodes\n",
    "        self.quantitative_nodes = list(set(data.columns) - set(self.qualitative_nodes))\n",
    "\n",
    "        self.gamma_max = gamma_max\n",
    "        self.sig_level = sig_level\n",
    "        self.threshold_for_discretization_dict = threshold_for_discretization_dict\n",
    "        self.test_threshold_for_discretization()\n",
    "\n",
    "        self.nodes_to_tnodes = dict()\n",
    "        self.tnodes_to_thresholded_tnodes = dict()\n",
    "        self.thresholded_tnodes_to_thresholded_nodes = dict()\n",
    "        self.time_table = dict()\n",
    "        self.tnodes = []\n",
    "        for node in data.columns:\n",
    "            self.nodes_to_tnodes[node] = []\n",
    "            for gamma in range(2 * self.gamma_max + 1):\n",
    "                if gamma == 0:\n",
    "                    temporal_node = str(node) + \"_t\"\n",
    "                    self.nodes_to_tnodes[node].append(temporal_node)\n",
    "                    self.tnodes.append(temporal_node)\n",
    "                    self.time_table[temporal_node] = gamma\n",
    "                else:\n",
    "                    temporal_node = str(node) + \"_t_\" + str(gamma)\n",
    "                    self.nodes_to_tnodes[node].append(temporal_node)\n",
    "                    self.tnodes.append(temporal_node)\n",
    "                    self.time_table[temporal_node] = gamma\n",
    "\n",
    "                ###################################################################################\n",
    "                # create thresholded nodes names\n",
    "                if len(threshold_for_discretization_dict[node]) > 1:\n",
    "                    threshold_col_list = self.threshold_for_discretization_dict[node].copy()\n",
    "                    threshold_col_list.sort()\n",
    "                    for th in range(len(threshold_col_list)):\n",
    "                        if th == 0:\n",
    "                            thresholded_tnode = str(temporal_node) + \"<\" + str(round(threshold_col_list[th], ndigits=2))\n",
    "                            thresholded_node = str(node) + \"<\" + str(round(threshold_col_list[th], ndigits=2))\n",
    "                        else:\n",
    "                            thresholded_tnode = str(temporal_node) + \">\" + str(round(threshold_col_list[th], ndigits=2))\n",
    "                            thresholded_node = str(node) + \">\" + str(round(threshold_col_list[th], ndigits=2))\n",
    "                        if temporal_node in self.tnodes_to_thresholded_tnodes.keys():\n",
    "                            self.tnodes_to_thresholded_tnodes[temporal_node].append(thresholded_tnode)\n",
    "                        else:\n",
    "                            self.tnodes_to_thresholded_tnodes[temporal_node] = [thresholded_tnode]\n",
    "                        self.thresholded_tnodes_to_thresholded_nodes[thresholded_tnode] = thresholded_node\n",
    "                else:\n",
    "                    thresholded_tnode = str(temporal_node) + \">\" + str(round(self.threshold_for_discretization_dict[node][0], ndigits=2))\n",
    "                    thresholded_node = str(node) + \">\" + str(round(self.threshold_for_discretization_dict[node][0], ndigits=2))\n",
    "                    if temporal_node in self.tnodes_to_thresholded_tnodes.keys():\n",
    "                        self.tnodes_to_thresholded_tnodes[temporal_node].append(thresholded_tnode)\n",
    "                    else:\n",
    "                        self.tnodes_to_thresholded_tnodes[temporal_node] = [thresholded_tnode]\n",
    "                    self.thresholded_tnodes_to_thresholded_nodes[thresholded_tnode] = thresholded_node\n",
    "                ###################################################################################\n",
    "\n",
    "        # create inverse dicts\n",
    "        ###################################################################################\n",
    "        self.tnodes_to_nodes = {v: k for k, v_list in self.nodes_to_tnodes.items() for v in v_list}\n",
    "        self.thresholded_tnodes_to_tnodes = {v: k for k, v_list in\n",
    "                                                    self.tnodes_to_thresholded_tnodes.items() for v in v_list}\n",
    "        self.tnodes_or_thresholded_tnodes_to_nodes = self.tnodes_to_nodes.copy()\n",
    "        for thresholded_nodes in self.thresholded_tnodes_to_tnodes.keys():\n",
    "            self.tnodes_or_thresholded_tnodes_to_nodes[thresholded_nodes] = \\\n",
    "                self.tnodes_to_nodes[self.thresholded_tnodes_to_tnodes[thresholded_nodes]]\n",
    "        ###################################################################################\n",
    "\n",
    "        # print('#####################################')\n",
    "        # print(self.tnodes)\n",
    "        # print(self.nodes_to_tnodes)\n",
    "        # print(self.tnodes_to_nodes)\n",
    "        # print(self.tnodes_to_thresholded_tnodes)\n",
    "        # print(self.thresholded_tnodes_to_tnodes)\n",
    "        # print(self.tnodes_or_thresholded_tnodes_to_nodes)\n",
    "        # print('#####################################')\n",
    "\n",
    "        self.normal_nodes = []\n",
    "        self.anomalous_nodes = []\n",
    "        self.data = self._process_data(data)\n",
    "        self.discretized_data = self._quantitative_to_qualitative()\n",
    "\n",
    "        self.prima_facie_causes = dict()\n",
    "        self.genuine_causes = dict()\n",
    "        self.root_causes = []\n",
    "\n",
    "    def test_threshold_for_discretization(self):\n",
    "        for k in self.threshold_for_discretization_dict.keys():\n",
    "            if len(self.threshold_for_discretization_dict[k]) > 2:\n",
    "                print(\"Error: too many thresholds for time series (\" + str(k) + \"). Max thresholds allowed is 2.\")\n",
    "                exit(0)\n",
    "\n",
    "    # Todo: tansform series with mutltiple threshold into many binary variables (with true false values)\n",
    "    def _quantitative_to_qualitative(self):\n",
    "        discretized_variables = []\n",
    "        for tnode in self.data.columns:\n",
    "            if self.tnodes_to_nodes[tnode] in self.quantitative_nodes:\n",
    "                discretized_variables.append(tnode)\n",
    "        discretized_data = pd.DataFrame(columns=discretized_variables)\n",
    "        tnodes_to_eliminate = []\n",
    "        for tnode in discretized_data.columns:\n",
    "            discretized_data_col = []\n",
    "            threshold_col_list = self.threshold_for_discretization_dict[self.tnodes_to_nodes[tnode]].copy()\n",
    "            threshold_col_list.sort()\n",
    "            if len(threshold_col_list) == 1:\n",
    "                for v in self.data[tnode]:\n",
    "                    if v > threshold_col_list[0]:\n",
    "                        discretized_data_col.append(True)\n",
    "                    else:\n",
    "                        discretized_data_col.append(False)\n",
    "\n",
    "                if len(set(discretized_data_col)) == 1:\n",
    "                    self.normal_nodes.append(self.tnodes_to_thresholded_tnodes[tnode][0])\n",
    "                else:\n",
    "                    self.anomalous_nodes.append(self.tnodes_to_thresholded_tnodes[tnode][0])\n",
    "                discretized_data[self.tnodes_to_thresholded_tnodes[tnode][0]] = discretized_data_col\n",
    "            else:\n",
    "                for v in self.data[tnode]:\n",
    "                    #######################################################################################\n",
    "                    # first approach: distinguish between top outliers and down outliers\n",
    "                    one_vs_many = [False] * 2\n",
    "                    if v < threshold_col_list[0]:\n",
    "                        one_vs_many[0] = True\n",
    "                    elif v > threshold_col_list[1]:\n",
    "                        one_vs_many[1] = True\n",
    "                    discretized_data_col.append(one_vs_many)\n",
    "                    #######################################################################################\n",
    "                    # second approach: treat top outliers and down outliers as the same outlier\n",
    "                    # if (v < threshold_col_list[0]) or (v > threshold_col_list[1]):\n",
    "                    #     discretized_data_col.append(True)\n",
    "                    # else:\n",
    "                    #     discretized_data_col.append(False)\n",
    "                    #######################################################################################\n",
    "                discretized_data_col = np.array(discretized_data_col)\n",
    "                for i in range(len(threshold_col_list)):\n",
    "                    if len(set(discretized_data_col[:, i])) != 1:\n",
    "                        discretized_data[self.tnodes_to_thresholded_tnodes[tnode][i]] = \\\n",
    "                            discretized_data_col[:, i]\n",
    "                    if len(set(discretized_data_col[:, i])) == 1:\n",
    "                        self.normal_nodes.append(self.tnodes_to_thresholded_tnodes[tnode][i])\n",
    "                    else:\n",
    "                        self.anomalous_nodes.append(self.tnodes_to_thresholded_tnodes[tnode][i])\n",
    "            if tnode not in tnodes_to_eliminate:\n",
    "                tnodes_to_eliminate.append(tnode)\n",
    "        discretized_data.drop(tnodes_to_eliminate, axis=1, inplace=True)\n",
    "        return discretized_data\n",
    "\n",
    "    def _process_data(self, data):\n",
    "        new_data = pd.DataFrame()\n",
    "        # for gamma in range(0, 2 * self.gamma_max + 1):\n",
    "        #     shifted_data = data.shift(periods=-2 * self.gamma_max + gamma)\n",
    "        for gamma in range(0, self.gamma_max + 1):\n",
    "            shifted_data = data.shift(periods=self.gamma_max + gamma)\n",
    "            new_columns = []\n",
    "            for node in data.columns:\n",
    "                new_columns.append(self.nodes_to_tnodes[node][gamma])\n",
    "            shifted_data.columns = new_columns\n",
    "            new_data = pd.concat([new_data, shifted_data], axis=1, join=\"outer\")\n",
    "        new_data.dropna(axis=0, inplace=True)\n",
    "        return new_data\n",
    "\n",
    "    def is_prima_facie(self, temporal_effect, temporal_or_thresholded_cause):\n",
    "        effect_value = list(self.data[temporal_effect])\n",
    "\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[temporal_or_thresholded_cause] in self.quantitative_nodes:\n",
    "            cause_value = list(self.discretized_data[temporal_or_thresholded_cause])\n",
    "        else:\n",
    "            cause_value = list(self.data[temporal_or_thresholded_cause])\n",
    "\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[temporal_effect] in self.quantitative_nodes:\n",
    "            mean_e = mean(effect_value)\n",
    "            list_e_c = []\n",
    "            for (c, e) in zip(cause_value, effect_value):\n",
    "                if c == True:\n",
    "                    list_e_c.append(e)\n",
    "            # return(stats.ttest_ind(effect_value, list_e_c, permutations=500, equal_var=False)[1] <= 0.05)\n",
    "            return mean(list_e_c) != mean_e\n",
    "        else:\n",
    "            c_and_e = sum([c and e for (c, e) in zip(cause_value, effect_value)])\n",
    "            c_true = sum(cause_value)\n",
    "            e_true = sum(effect_value)\n",
    "            events = len(effect_value)\n",
    "            if c_true == 0:\n",
    "                return False\n",
    "            result = (c_and_e / c_true) != (e_true / events)\n",
    "            return result\n",
    "\n",
    "    def find_prima_facie_causes(self):\n",
    "        nodes_t = [node for node in self.data.columns if self.time_table[node] == 0]\n",
    "        for temporal_effect in nodes_t:\n",
    "            for temporal_cause in self.data.columns:\n",
    "                # condition on temporal priority between cause and effect\n",
    "                if self.time_table[temporal_effect] - self.time_table[temporal_cause] < 0:\n",
    "                    if temporal_effect != temporal_cause:\n",
    "                        # take into account multi thresholding\n",
    "                        if temporal_cause in self.tnodes_to_thresholded_tnodes.keys():\n",
    "                            list_temporal_or_thresholded_causes = \\\n",
    "                                self.tnodes_to_thresholded_tnodes[temporal_cause]\n",
    "                        else:\n",
    "                            list_temporal_or_thresholded_causes = [temporal_cause]\n",
    "                        for temporal_or_thresholded_cause in list_temporal_or_thresholded_causes:\n",
    "                            if temporal_or_thresholded_cause in self.anomalous_nodes:\n",
    "                                if self.is_prima_facie(temporal_effect, temporal_or_thresholded_cause):\n",
    "                                    if temporal_effect in self.prima_facie_causes:\n",
    "                                        self.prima_facie_causes[temporal_effect].append(temporal_or_thresholded_cause)\n",
    "                                    else:\n",
    "                                        self.prima_facie_causes[temporal_effect] = [temporal_or_thresholded_cause]\n",
    "\n",
    "    def get_other_causes(self, temporal_or_thresholded_cause, temporal_effect):\n",
    "        # if temporal_or_thresholded_cause in self.thresholded_tnodes_to_tnodes.keys():\n",
    "        #     temporal_cause = self.thresholded_tnodes_to_tnodes[temporal_or_thresholded_cause]\n",
    "        #     main_cause = self.tnodes_to_thresholded_tnodes[temporal_cause]\n",
    "        # else:\n",
    "        #     main_cause = [temporal_or_thresholded_cause]\n",
    "        main_cause = [temporal_or_thresholded_cause]\n",
    "        other_causes = [cause for cause in self.prima_facie_causes[temporal_effect] if\n",
    "                        cause not in main_cause]\n",
    "        return other_causes\n",
    "\n",
    "    def calculate_probability_difference_2011_no_x(self, temporal_or_thresholded_cause, temporal_effect):\n",
    "        effect_value = list(self.data[temporal_effect])\n",
    "\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[temporal_or_thresholded_cause] in self.quantitative_nodes:\n",
    "            cause_value = list(self.discretized_data[temporal_or_thresholded_cause])\n",
    "        else:\n",
    "            cause_value = list(self.data[temporal_or_thresholded_cause])\n",
    "\n",
    "        if self.tnodes_to_nodes[temporal_effect] in self.qualitative_nodes:\n",
    "            e_and_c = [e and c for (e, c) in zip(effect_value, cause_value)]\n",
    "\n",
    "            e_and_not_c = [e and (not c) for (e, c) in zip(effect_value, cause_value)]\n",
    "\n",
    "            return sum(e_and_c) - sum(e_and_not_c)\n",
    "        else:\n",
    "            list_e_c = []\n",
    "            for (e, cx) in zip(effect_value, cause_value):\n",
    "                if cx == True:\n",
    "                    list_e_c.append(e)\n",
    "\n",
    "            not_c = [(not c) for c in cause_value]\n",
    "            list_e_not_c = []\n",
    "            for (e, ncx) in zip(effect_value, not_c):\n",
    "                if ncx == True:\n",
    "                    list_e_not_c.append(e)\n",
    "            if len(list_e_c) == 0:\n",
    "                mean_e_c = 0\n",
    "            else:\n",
    "                mean_e_c = mean(list_e_c)\n",
    "            if len(list_e_not_c) == 0:\n",
    "                mean_e_not_c = 0\n",
    "            else:\n",
    "                mean_e_not_c = mean(list_e_not_c)\n",
    "            return mean_e_c - mean_e_not_c\n",
    "\n",
    "    def calculate_probability_difference_2011(self, temporal_or_thresholded_cause, temporal_effect, x):\n",
    "        effect_value = list(self.data[temporal_effect])\n",
    "\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[temporal_or_thresholded_cause] in self.quantitative_nodes:\n",
    "            cause_value = list(self.discretized_data[temporal_or_thresholded_cause])\n",
    "        else:\n",
    "            cause_value = list(self.data[temporal_or_thresholded_cause])\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[x] in self.quantitative_nodes:\n",
    "            x_value = list(self.discretized_data[x])\n",
    "        else:\n",
    "            x_value = list(self.data[x])\n",
    "\n",
    "        if self.tnodes_to_nodes[temporal_effect] in self.qualitative_nodes:\n",
    "            c_and_x = [c and x for (c, x) in zip(cause_value, x_value)]\n",
    "            e_and_c_and_x = [e and cx for (e, cx) in zip(effect_value, c_and_x)]\n",
    "\n",
    "            not_c_and_x = [(not c) and x for (c, x) in zip(cause_value, x_value)]\n",
    "            e_and_not_c_and_x = [e and ncx for (e, ncx) in zip(effect_value, not_c_and_x)]\n",
    "\n",
    "            if sum(c_and_x) == 0 or sum(not_c_and_x) == 0:\n",
    "                return None\n",
    "\n",
    "            return sum(e_and_c_and_x) / sum(c_and_x) - sum(e_and_not_c_and_x) / sum(not_c_and_x)\n",
    "        else:\n",
    "            c_and_x = [c and x for (c, x) in zip(cause_value, x_value)]\n",
    "            list_e_c = []\n",
    "            for (e, cx) in zip(effect_value, c_and_x):\n",
    "                if cx == True:\n",
    "                    list_e_c.append(e)\n",
    "\n",
    "            not_c_and_x = [(not c) and x for (c, x) in zip(cause_value, x_value)]\n",
    "            list_e_not_c = []\n",
    "            for (e, ncx) in zip(effect_value, not_c_and_x):\n",
    "                if ncx == True:\n",
    "                    list_e_not_c.append(e)\n",
    "            if len(list_e_c) == 0:\n",
    "                mean_e_c = 0\n",
    "            else:\n",
    "                mean_e_c = mean(list_e_c)\n",
    "            if len(list_e_not_c) == 0:\n",
    "                mean_e_not_c = 0\n",
    "            else:\n",
    "                mean_e_not_c = mean(list_e_not_c)\n",
    "            return mean_e_c - mean_e_not_c\n",
    "\n",
    "    # Auxiliary method\n",
    "    def get_epsilon_average_2011(self, temporal_or_thresholded_cause, temporal_effect):\n",
    "        other_causes = self.get_other_causes(temporal_or_thresholded_cause, temporal_effect)\n",
    "        eps_x = 0\n",
    "        if len(other_causes) != 0:\n",
    "            for x in other_causes:\n",
    "                eps_result = self.calculate_probability_difference_2011(temporal_or_thresholded_cause, temporal_effect,\n",
    "                                                                        x)\n",
    "                if eps_result == None:\n",
    "                    return None\n",
    "                eps_x += eps_result\n",
    "            eps_avg = eps_x / len(other_causes)\n",
    "            return eps_avg\n",
    "        else:\n",
    "            return self.calculate_probability_difference_2011_no_x(temporal_or_thresholded_cause, temporal_effect)\n",
    "\n",
    "    # Main function of logic-based method of 2011\n",
    "    def do_all_epsilon_averages_2011(self):\n",
    "        list_epsilon = {}\n",
    "        for temporal_effect in self.prima_facie_causes:\n",
    "            for temporal_or_thresholded_cause in self.prima_facie_causes[temporal_effect]:\n",
    "                list_epsilon[(temporal_or_thresholded_cause, temporal_effect)] = \\\n",
    "                    self.get_epsilon_average_2011(temporal_or_thresholded_cause, temporal_effect)\n",
    "        return list_epsilon\n",
    "\n",
    "    def find_genuine_causes(self, all_epsilon_averages):\n",
    "        for ce in all_epsilon_averages.keys():\n",
    "            if abs(all_epsilon_averages[ce]) >= self.sig_level:\n",
    "                if ce[1] in self.genuine_causes:\n",
    "                    self.genuine_causes[ce[1]].append(ce[0])\n",
    "                else:\n",
    "                    self.genuine_causes[ce[1]] = [ce[0]]\n",
    "                    \n",
    "    def find_prob_of_root_causes(self, epsilon):\n",
    "        graph = self.construct_summary_graph(plot=False)\n",
    "        prob_root = {}\n",
    "        for child in graph.nodes:\n",
    "            prob_root[child] = {}\n",
    "            prob_root[child][child] = pow(epsilon,len(list(graph.predecessors(child))))\n",
    "            for root in nx.ancestors(graph, child):\n",
    "                all_paths = list(nx.all_simple_paths(graph, source=root, target=child))\n",
    "                prob = 0\n",
    "                for path in all_paths:\n",
    "                    prob += pow(1-epsilon,len(path)-1)\n",
    "                if len(list(graph.predecessors(root))) != 0:\n",
    "                    prob *= pow(epsilon,len(list(graph.predecessors(root))))\n",
    "                prob_root[child][root] = prob\n",
    "            z = sum(prob_root[child].values())\n",
    "            for root in prob_root[child].keys():\n",
    "                prob_root[child][root] = prob_root[child][root]/z\n",
    "        return prob_root\n",
    "    \n",
    "    # todo\n",
    "    def find_root_causes_of_anomalies(self):\n",
    "        summary_graph = self.construct_summary_graph(plot=True)\n",
    "        for node in summary_graph.nodes:\n",
    "            parents_of_node = list(summary_graph.predecessors(node))\n",
    "            if len(parents_of_node) == 0:\n",
    "                self.root_causes.append(node)\n",
    "            else:\n",
    "                if (len(parents_of_node) == 1) and parents_of_node[0] == node:\n",
    "                    self.root_causes.append(node)\n",
    "\n",
    "    def construct_temporal_graph(self, plot=True):\n",
    "        list_nodes_1 = []\n",
    "        list_edges_1 = []\n",
    "\n",
    "        for temporal_node in self.tnodes:\n",
    "            if self.time_table[temporal_node] == 0:\n",
    "                list_nodes_1.append(temporal_node)\n",
    "\n",
    "        for effect in self.genuine_causes.keys():\n",
    "            for cause in self.genuine_causes[effect]:\n",
    "                if cause in self.thresholded_tnodes_to_tnodes.keys():\n",
    "                    cause = self.thresholded_tnodes_to_tnodes[cause]\n",
    "                if cause not in list_nodes_1:\n",
    "                    list_nodes_1.append(cause)\n",
    "                if (cause, effect) not in list_edges_1:\n",
    "                    list_edges_1.append((cause, effect))\n",
    "\n",
    "        temporal_graph = nx.DiGraph()\n",
    "        temporal_graph.add_nodes_from(list_nodes_1)\n",
    "        temporal_graph.add_edges_from(list_edges_1)\n",
    "        pos = dict()\n",
    "        all_nodes = self.qualitative_nodes + self.quantitative_nodes\n",
    "        for temporal_node in list_nodes_1:\n",
    "            node = self.tnodes_to_nodes[temporal_node]\n",
    "            y = all_nodes.index(node)\n",
    "            pos[temporal_node] = [-self.time_table[temporal_node], y]\n",
    "\n",
    "        if plot:\n",
    "            nx.draw(temporal_graph, pos, with_labels=True)\n",
    "            plt.show()\n",
    "        return temporal_graph\n",
    "\n",
    "    def construct_summary_graph(self, plot=True):\n",
    "        list_nodes_1 = []\n",
    "        list_edges_1 = []\n",
    "        for node in self.qualitative_nodes + self.quantitative_nodes:\n",
    "            list_nodes_1.append(node)\n",
    "\n",
    "        for effect_t in self.genuine_causes.keys():\n",
    "            for cause_t in self.genuine_causes[effect_t]:\n",
    "                effect = self.tnodes_to_nodes[effect_t]\n",
    "                cause = self.tnodes_or_thresholded_tnodes_to_nodes[cause_t]\n",
    "                if (cause, effect) not in list_edges_1:\n",
    "                    list_edges_1.append((cause, effect))\n",
    "\n",
    "        summary_graph = nx.DiGraph()\n",
    "        summary_graph.add_nodes_from(list_nodes_1)\n",
    "        summary_graph.add_edges_from(list_edges_1)\n",
    "        if plot:\n",
    "            nx.draw(summary_graph, with_labels=True)\n",
    "            plt.show()\n",
    "        return summary_graph\n",
    "\n",
    "    def construct_temporal_outlier_graph(self, plot=True):\n",
    "        # todo adapt to multi threshold\n",
    "        list_nodes_0 = []\n",
    "        # list_nodes_1 = []\n",
    "        list_edges_1 = []\n",
    "        for temporal_node in self.tnodes:\n",
    "            if self.time_table[temporal_node] == 0:\n",
    "                list_nodes_0.append(temporal_node)\n",
    "\n",
    "        for effect_t in self.genuine_causes.keys():\n",
    "            for cause_tt in self.genuine_causes[effect_t]:\n",
    "                list_nodes_0.append(cause_tt)\n",
    "                list_edges_1.append((cause_tt, effect_t))\n",
    "\n",
    "        temporal_outlier_graph = nx.DiGraph()\n",
    "        temporal_outlier_graph.add_nodes_from(list_nodes_0)\n",
    "        temporal_outlier_graph.add_edges_from(list_edges_1)\n",
    "        pos = dict()\n",
    "        all_nodes = []\n",
    "        all_thresholded_nodes = dict()\n",
    "        for t in range(1, self.gamma_max + 1):\n",
    "            all_thresholded_nodes[t] = []\n",
    "        for node in self.qualitative_nodes + self.quantitative_nodes:\n",
    "            for node_t_ in self.nodes_to_tnodes[node]:\n",
    "                if 0 < self.time_table[node_t_] <= self.gamma_max:\n",
    "                    if node_t_ in self.tnodes_to_thresholded_tnodes.keys():\n",
    "                        all_thresholded_nodes[self.time_table[node_t_]] = \\\n",
    "                            all_thresholded_nodes[self.time_table[node_t_]] + \\\n",
    "                            self.tnodes_to_thresholded_tnodes[node_t_] + [None]\n",
    "                        if node not in all_nodes:\n",
    "                            all_nodes = all_nodes + [node] * len(self.tnodes_to_thresholded_tnodes[node_t_]) + [None]\n",
    "                    else:\n",
    "                        all_thresholded_nodes[self.time_table[node_t_]] = \\\n",
    "                            all_thresholded_nodes[self.time_table[node_t_]] + [node_t_] + [None]\n",
    "                        if node not in all_nodes:\n",
    "                            all_nodes = all_nodes + [node] + [None]\n",
    "        for node_tt in list_nodes_0:\n",
    "            node = self.tnodes_or_thresholded_tnodes_to_nodes[node_tt]\n",
    "            if node_tt in self.thresholded_tnodes_to_tnodes.keys():\n",
    "                node_t_ = self.thresholded_tnodes_to_tnodes[node_tt]\n",
    "            else:\n",
    "                node_t_ = node_tt\n",
    "            if self.time_table[node_t_] > 0:\n",
    "                y = all_thresholded_nodes[self.time_table[node_t_]].index(node_tt)\n",
    "            else:\n",
    "                y = all_nodes.index(node)\n",
    "            pos[node_tt] = [-self.time_table[node_t_], y]\n",
    "        if plot:\n",
    "            nx.draw(temporal_outlier_graph, pos, with_labels=True)\n",
    "            plt.show()\n",
    "        return temporal_outlier_graph\n",
    "\n",
    "    def construct_outlier_graph(self, plot=True):\n",
    "        list_effects = []\n",
    "        list_causes = []\n",
    "        list_edges_1 = []\n",
    "        for node in self.qualitative_nodes + self.quantitative_nodes:\n",
    "            list_effects.append(node)\n",
    "\n",
    "        processed_causes_to_causes = dict()\n",
    "        for teffect in self.genuine_causes.keys():\n",
    "            effect = self.tnodes_to_nodes[teffect]\n",
    "            for tnodes_or_thresholded_tnodes in self.genuine_causes[teffect]:\n",
    "                cause = self.tnodes_or_thresholded_tnodes_to_nodes[tnodes_or_thresholded_tnodes]\n",
    "                if len(self.threshold_for_discretization_dict[cause]) > 0:\n",
    "                    processed_cause = self.thresholded_tnodes_to_thresholded_nodes[tnodes_or_thresholded_tnodes]\n",
    "                else:\n",
    "                    processed_cause = cause + \"--\"\n",
    "                processed_causes_to_causes[processed_cause] = cause\n",
    "                if processed_cause not in list_causes:\n",
    "                    list_causes.append(processed_cause)\n",
    "                if (processed_cause, effect) not in list_edges_1:\n",
    "                    list_edges_1.append((processed_cause, effect))\n",
    "\n",
    "        outlier_graph = nx.DiGraph()\n",
    "        outlier_graph.add_nodes_from(list_effects)\n",
    "        outlier_graph.add_nodes_from(list_causes)\n",
    "        outlier_graph.add_edges_from(list_edges_1)\n",
    "\n",
    "        pos = dict()\n",
    "        for effect in list_effects:\n",
    "            # cause = processed_causes_to_causes[processed_cause]\n",
    "            y = list_effects.index(effect)\n",
    "            pos[effect] = [1, y]\n",
    "        for processed_cause in list_causes:\n",
    "            y = list_causes.index(processed_cause)\n",
    "            pos[processed_cause] = [0, y]\n",
    "\n",
    "        if plot:\n",
    "            nx.draw(outlier_graph, pos, with_labels=True)\n",
    "            # nx.draw(G, with_labels=True, pos=nx.drawing.layout.bipartite_layout(G, list_nodes_1),)\n",
    "            plt.show()\n",
    "        return outlier_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ecc03-14bd-475c-9a3c-ce1f505685c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RAITIA2015:\n",
    "    def __init__(self, data, categorical_nodes, gamma_max, sig_level, threshold_for_discretization_dict):\n",
    "        self.graph = nx.DiGraph()\n",
    "\n",
    "        self.data = data\n",
    "        self.qualitative_nodes = categorical_nodes\n",
    "        self.quantitative_nodes = list(set(data.columns) - set(self.qualitative_nodes))\n",
    "        \n",
    "        self.categorical_nodes = categorical_nodes\n",
    "        self.gamma_min = 1\n",
    "        self.gamma_max = gamma_max\n",
    "        self.sig_level = sig_level\n",
    "        self.threshold_for_discretization_dict = threshold_for_discretization_dict\n",
    "        self.dic_prima_facie_causes = None\n",
    "        self.column_to_index = {}\n",
    "        self.index_to_column = {}\n",
    "        for i in range(len(data.columns)):\n",
    "            self.column_to_index[data.columns[i]] = i\n",
    "            self.index_to_column[i] = data.columns[i]\n",
    "        \n",
    "    \n",
    "    def generate_dic_prima_facie_cause(self, root_cause):\n",
    "        dic_prima_facie_causes = {}\n",
    "        for node in root_cause.keys():\n",
    "            dic_prima_facie_causes[node.split('_')[0]] = []\n",
    "            for cause in root_cause[node]:\n",
    "                dic_prima_facie_causes[node.split('_')[0]].append(cause.split('_')[0])\n",
    "            dic_prima_facie_causes[node.split('_')[0]] = list(set(dic_prima_facie_causes[node.split('_')[0]]))\n",
    "            \n",
    "        return dic_prima_facie_causes\n",
    "\n",
    "    def return_ele_B(self, cause, thres_cause, effect, lag_min, lag_max):\n",
    "        T_e = sampling_number = len(effect)\n",
    "        T_e_c = None\n",
    "        N_e_c = None\n",
    "        E_e_c = None\n",
    "\n",
    "        list_E_e_c = []\n",
    "        list_N_e_c = []\n",
    "\n",
    "        index = 0\n",
    "        for i in cause:\n",
    "            if i >= thres_cause:\n",
    "                for lag in range(lag_min, lag_max+1):\n",
    "                    if index + lag < sampling_number:\n",
    "                        list_N_e_c.append(index+lag)\n",
    "                        list_E_e_c.append(effect[index+lag])\n",
    "            index+=1\n",
    "        N_e_c = len(list_N_e_c)\n",
    "        T_e_c = len(set(list_N_e_c))\n",
    "        E_e_c = mean(list_E_e_c)\n",
    "        E_e = mean(effect)\n",
    "\n",
    "        f_e_c = (T_e * T_e_c)/(N_e_c*(T_e - T_e_c))\n",
    "\n",
    "        return f_e_c*(E_e_c-E_e)\n",
    "\n",
    "\n",
    "    def return_ele_A(self, cause, thres_cause, x, thres_x, effect, lag_min, lag_max):\n",
    "        T_e = sampling_number = len(effect)\n",
    "        T_e_c = None\n",
    "        N_e_c = None\n",
    "        N_e_x = None\n",
    "        N_e_c_x = 0\n",
    "\n",
    "        list_N_e_c = []\n",
    "        list_N_e_x = []\n",
    "\n",
    "        index = 0\n",
    "        for i in cause:\n",
    "            if i >= thres_cause:\n",
    "                for lag in range(lag_min, lag_max+1):\n",
    "                    if index + lag < sampling_number:\n",
    "                        list_N_e_c.append(index+lag)\n",
    "            index+=1\n",
    "\n",
    "        list_T_e_c = set(list_N_e_c)\n",
    "        N_e_c = len(list_N_e_c)\n",
    "        T_e_c = len(list_T_e_c)\n",
    "\n",
    "        index = 0\n",
    "        for i in x:\n",
    "            win_eles = []\n",
    "            if i >= thres_x:\n",
    "                for lag in range(lag_min, lag_max+1):\n",
    "                    if index + lag < sampling_number:\n",
    "                        list_N_e_x.append(index+lag)\n",
    "                        win_eles.append(index+lag)\n",
    "            N_e_c_x += len(set(win_eles) & list_T_e_c)\n",
    "            index+=1\n",
    "\n",
    "        N_e_x = len(list_N_e_x)\n",
    "\n",
    "        return (N_e_c_x*T_e - N_e_x*T_e_c)/(N_e_c*(T_e - T_e_c))\n",
    "\n",
    "\n",
    "    def causal_significance_estimation(self, data, poten_cause_index, thres_for_causes, effect_index, lag_min, lag_max):\n",
    "        num_poten_cause = len(poten_cause_index)\n",
    "        data_value = data.values\n",
    "\n",
    "        A = np.zeros(shape=(num_poten_cause, num_poten_cause))\n",
    "        B = np.zeros(shape=(num_poten_cause,1))\n",
    "\n",
    "        for i in range(num_poten_cause):\n",
    "            B[i,0] = self.return_ele_B(cause=data_value[:,i], thres_cause=thres_for_causes[i],\n",
    "                                  effect=data_value[:,effect_index], lag_min=lag_min, lag_max=lag_max)\n",
    "            for m in range(num_poten_cause):\n",
    "                A[i,m] = self.return_ele_A(cause=data_value[:,i], thres_cause=thres_for_causes[i],\n",
    "                                      x=data_value[:,m], thres_x=thres_for_causes[m],\n",
    "                                      effect=data_value[:,effect_index], lag_min=lag_min, lag_max=lag_max)\n",
    "\n",
    "        return np.matmul(inv(A),B)\n",
    "\n",
    "    def generate_outlier_graph(self):\n",
    "        # generate the outlier summary causal graph \n",
    "        outlier_graph = nx.DiGraph()\n",
    "        outlier_graph.add_nodes_from(self.data.columns)\n",
    "        \n",
    "        ai2011 = RAITIA2011(data=self.data, categorical_nodes=self.categorical_nodes, gamma_max=self.gamma_max, sig_level=self.sig_level,\n",
    "                        threshold_for_discretization_dict=self.threshold_for_discretization_dict)\n",
    "        ai2011.find_prima_facie_causes()\n",
    "        # print(ai2011.prima_facie_causes)\n",
    "        self.dic_prima_facie_causes = self.generate_dic_prima_facie_cause(root_cause=ai2011.prima_facie_causes)\n",
    "        \n",
    "        for effect in self.dic_prima_facie_causes.keys():\n",
    "            effect_index = self.column_to_index[effect]\n",
    "            poten_cause_index = [self.column_to_index[i] for i in self.dic_prima_facie_causes[effect] if i != effect]\n",
    "            # poten_cause_index = [self.column_to_index[i] for i in self.dic_prima_facie_causes[effect]]\n",
    "            thres_for_causes = [self.threshold_for_discretization_dict[i] for i in self.dic_prima_facie_causes[effect]]\n",
    "            vec_sig_level = self.causal_significance_estimation(data=self.data, poten_cause_index=poten_cause_index, \n",
    "                                                                thres_for_causes=thres_for_causes, effect_index=effect_index, \n",
    "                                                                lag_min=self.gamma_min, lag_max=self.gamma_max)\n",
    "            for link_index in range(vec_sig_level.shape[0]):\n",
    "                if np.abs(vec_sig_level[link_index,0]) >= self.sig_level:\n",
    "                    outlier_graph.add_edge(self.index_to_column[poten_cause_index[link_index]], effect)\n",
    "        return outlier_graph\n",
    "                    \n",
    "    def find_root_causes(self, outlier_graph):            \n",
    "        # find the root cause\n",
    "        list_root_causes = []\n",
    "        for node in outlier_graph.nodes: \n",
    "            if len(list(outlier_graph.predecessors(node))) == 0:\n",
    "                list_root_causes.append(node)\n",
    "        return list_root_causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "053bd59b-6dac-4c0d-ac3e-6e77f7b8af31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder_path = os.path.join('..', 'RCA_simulated_data', 'certain', 'data')\n",
    "data_files = [os.path.join(data_folder_path, f) for f in os.listdir(data_folder_path) if os.path.isfile(os.path.join(data_folder_path, f))]\n",
    "\n",
    "data_path = data_files[0]\n",
    "param_data = pd.read_csv(data_path)\n",
    "\n",
    "graph_path = os.path.join('..', 'RCA_simulated_data', 'certain', 'data_info', data_path.split('/')[-1].replace('data', 'info').replace('csv', 'json'))\n",
    "with open(graph_path, 'r') as json_file:\n",
    "    json_graph = json.load(json_file)\n",
    "param_threshold_dict = json_graph['nodes_thres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93cb332a-ddd7-4d20-953b-cd665f2ec6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ai5 = RAITIA2015(data=param_data, categorical_nodes=[], gamma_max=3, sig_level=0.05, threshold_for_discretization_dict=param_threshold_dict)\n",
    "# outlier_graph = ai5.generate_outlier_graph()\n",
    "# pred_root_causes= ai5.find_root_causes(outlier_graph=outlier_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c92209b-f942-40e1-ad08-a57a1e8083d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_t': ['a_t_1>0.71', 'b_t_1>0.84', 'd_t_1>0.75', 'c_t_1>0.86', 'e_t_1>0.89', 'f_t_1>0.87'], 'b_t': ['a_t_1>0.71', 'b_t_1>0.84', 'd_t_1>0.75', 'c_t_1>0.86', 'e_t_1>0.89', 'f_t_1>0.87'], 'd_t': ['a_t_1>0.71', 'b_t_1>0.84', 'd_t_1>0.75', 'c_t_1>0.86', 'e_t_1>0.89', 'f_t_1>0.87'], 'c_t': ['a_t_1>0.71', 'b_t_1>0.84', 'd_t_1>0.75', 'c_t_1>0.86', 'e_t_1>0.89', 'f_t_1>0.87'], 'e_t': ['a_t_1>0.71', 'b_t_1>0.84', 'd_t_1>0.75', 'c_t_1>0.86', 'e_t_1>0.89', 'f_t_1>0.87'], 'f_t': ['a_t_1>0.71', 'b_t_1>0.84', 'd_t_1>0.75', 'c_t_1>0.86', 'e_t_1>0.89', 'f_t_1>0.87']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:01<01:29,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_t': ['a_t_1>0.87', 'b_t_1>0.74', 'c_t_1>0.84', 'f_t_1>0.78', 'd_t_1>0.73', 'e_t_1>0.86'], 'b_t': ['a_t_1>0.87', 'b_t_1>0.74', 'c_t_1>0.84', 'f_t_1>0.78', 'd_t_1>0.73', 'e_t_1>0.86'], 'c_t': ['a_t_1>0.87', 'b_t_1>0.74', 'c_t_1>0.84', 'f_t_1>0.78', 'd_t_1>0.73', 'e_t_1>0.86'], 'f_t': ['a_t_1>0.87', 'b_t_1>0.74', 'c_t_1>0.84', 'f_t_1>0.78', 'd_t_1>0.73', 'e_t_1>0.86'], 'd_t': ['a_t_1>0.87', 'b_t_1>0.74', 'c_t_1>0.84', 'f_t_1>0.78', 'd_t_1>0.73', 'e_t_1>0.86'], 'e_t': ['a_t_1>0.87', 'b_t_1>0.74', 'c_t_1>0.84', 'f_t_1>0.78', 'd_t_1>0.73', 'e_t_1>0.86']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:03<01:26,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_t': ['a_t_1>0.88', 'b_t_1>0.76', 'f_t_1>0.7', 'c_t_1>0.84', 'd_t_1>0.78', 'e_t_1>0.89'], 'b_t': ['a_t_1>0.88', 'b_t_1>0.76', 'f_t_1>0.7', 'c_t_1>0.84', 'd_t_1>0.78', 'e_t_1>0.89'], 'f_t': ['a_t_1>0.88', 'b_t_1>0.76', 'f_t_1>0.7', 'c_t_1>0.84', 'd_t_1>0.78', 'e_t_1>0.89'], 'c_t': ['a_t_1>0.88', 'b_t_1>0.76', 'f_t_1>0.7', 'c_t_1>0.84', 'd_t_1>0.78', 'e_t_1>0.89'], 'd_t': ['a_t_1>0.88', 'b_t_1>0.76', 'f_t_1>0.7', 'c_t_1>0.84', 'd_t_1>0.78', 'e_t_1>0.89'], 'e_t': ['a_t_1>0.88', 'b_t_1>0.76', 'f_t_1>0.7', 'c_t_1>0.84', 'd_t_1>0.78', 'e_t_1>0.89']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:04<01:51,  2.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m true_root_causes \u001b[38;5;241m=\u001b[39m data_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintervention_node\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m ai5 \u001b[38;5;241m=\u001b[39m RAITIA2015(data\u001b[38;5;241m=\u001b[39mparam_data, categorical_nodes\u001b[38;5;241m=\u001b[39mcategorical_nodes, gamma_max\u001b[38;5;241m=\u001b[39mgamma_max, sig_level\u001b[38;5;241m=\u001b[39msig_level, threshold_for_discretization_dict\u001b[38;5;241m=\u001b[39mparam_threshold_dict)\n\u001b[0;32m---> 35\u001b[0m outlier_graph \u001b[38;5;241m=\u001b[39m \u001b[43mai5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_outlier_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m pred_root_causes\u001b[38;5;241m=\u001b[39m ai5\u001b[38;5;241m.\u001b[39mfind_root_causes(outlier_graph\u001b[38;5;241m=\u001b[39moutlier_graph)\n\u001b[1;32m     38\u001b[0m pre, recall \u001b[38;5;241m=\u001b[39m cal_precision_recall(ground_truth\u001b[38;5;241m=\u001b[39mtrue_root_causes, predicion\u001b[38;5;241m=\u001b[39mpred_root_causes)\n",
      "Cell \u001b[0;32mIn[67], line 130\u001b[0m, in \u001b[0;36mRAITIA2015.generate_outlier_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m poten_cause_index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_to_index[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic_prima_facie_causes[effect]]\n\u001b[1;32m    129\u001b[0m thres_for_causes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold_for_discretization_dict[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic_prima_facie_causes[effect]]\n\u001b[0;32m--> 130\u001b[0m vec_sig_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal_significance_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoten_cause_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoten_cause_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mthres_for_causes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthres_for_causes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffect_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meffect_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mlag_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(vec_sig_level\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mabs(vec_sig_level[link_index,\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msig_level:\n",
      "Cell \u001b[0;32mIn[67], line 108\u001b[0m, in \u001b[0;36mRAITIA2015.causal_significance_estimation\u001b[0;34m(self, data, poten_cause_index, thres_for_causes, effect_index, lag_min, lag_max)\u001b[0m\n\u001b[1;32m    105\u001b[0m     B[i,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ele_B(cause\u001b[38;5;241m=\u001b[39mdata_value[:,i], thres_cause\u001b[38;5;241m=\u001b[39mthres_for_causes[i],\n\u001b[1;32m    106\u001b[0m                           effect\u001b[38;5;241m=\u001b[39mdata_value[:,effect_index], lag_min\u001b[38;5;241m=\u001b[39mlag_min, lag_max\u001b[38;5;241m=\u001b[39mlag_max)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_poten_cause):\n\u001b[0;32m--> 108\u001b[0m         A[i,m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_ele_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_value\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthres_cause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthres_for_causes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_value\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthres_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthres_for_causes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                              \u001b[49m\u001b[43meffect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_value\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43meffect_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlag_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlag_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmatmul(inv(A),B)\n",
      "Cell \u001b[0;32mIn[67], line 71\u001b[0m, in \u001b[0;36mRAITIA2015.return_ele_A\u001b[0;34m(self, cause, thres_cause, x, thres_x, effect, lag_min, lag_max)\u001b[0m\n\u001b[1;32m     69\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cause:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthres_cause\u001b[49m:\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(lag_min, lag_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m+\u001b[39m lag \u001b[38;5;241m<\u001b[39m sampling_number:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gamma_max = 1\n",
    "\n",
    "data_folder_path = os.path.join('..', 'RCA_simulated_data', 'certain', 'historical_data')\n",
    "data_files = [os.path.join(data_folder_path, f) for f in os.listdir(data_folder_path) if os.path.isfile(os.path.join(data_folder_path, f))]\n",
    "\n",
    "\n",
    "def cal_precision_recall(ground_truth, predicion):\n",
    "    pred_num = len(predicion)\n",
    "    truth_num = len(ground_truth)\n",
    "\n",
    "    true_pred = 0\n",
    "    for node in predicion:\n",
    "        if node in ground_truth:\n",
    "            true_pred+=1\n",
    "    if pred_num == 0:\n",
    "        return (0, true_pred/truth_num)\n",
    "    else:\n",
    "        return (true_pred/pred_num, true_pred/truth_num)\n",
    "\n",
    "res = {}\n",
    "for sig_level in np.arange(0.05, 0.55, 0.05).tolist():\n",
    "    Pre = []\n",
    "    Recall = []\n",
    "    F1 = []\n",
    "    for data_path in tqdm(data_files):\n",
    "        categorical_nodes = []\n",
    "        param_data = pd.read_csv(data_path)\n",
    "        data_info = os.path.join('..', 'RCA_simulated_data', 'certain', 'data_info', data_path.split('/')[-1].replace('data', 'info').replace('csv', 'json'))\n",
    "        with open(data_info, 'r') as json_file:\n",
    "            data_info = json.load(json_file)\n",
    "        param_threshold_dict = data_info['nodes_thres']\n",
    "        true_root_causes = data_info['intervention_node']\n",
    "\n",
    "        ai5 = RAITIA2015(data=param_data, categorical_nodes=categorical_nodes, gamma_max=gamma_max, sig_level=sig_level, threshold_for_discretization_dict=param_threshold_dict)\n",
    "        outlier_graph = ai5.generate_outlier_graph()\n",
    "        pred_root_causes= ai5.find_root_causes(outlier_graph=outlier_graph)\n",
    "\n",
    "        pre, recall = cal_precision_recall(ground_truth=true_root_causes, predicion=pred_root_causes)\n",
    "        Pre.append(pre)\n",
    "        Recall.append(recall)\n",
    "        if pre+recall == 0:\n",
    "            F1.append(0)\n",
    "        else:\n",
    "            F1.append(2*pre*recall/(pre+recall))\n",
    "\n",
    "    res[str(sig_level)] = (np.mean(Pre), np.mean(Recall), np.mean(F1))\n",
    "    print('precison: ' + str(np.mean(Pre)))\n",
    "    print('recall: ' + str(np.mean(Recall)))\n",
    "    print('F1: ' + str(np.mean(F1)))\n",
    "\n",
    "# res_path = os.path.join('..', 'Results', '2015_varying_sig_level.json')\n",
    "# with open(res_path, 'w') as json_file:\n",
    "#     json.dump(res, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a93035-5783-47fe-872d-355e21d4f9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
