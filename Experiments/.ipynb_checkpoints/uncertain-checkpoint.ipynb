{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0e9b6a-d027-457e-9e6d-b4e6f8c01346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os \n",
    "from statistics import mean\n",
    "import networkx as nx   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2707a860-8d24-4ba2-b158-a237a74ba08f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RAITIA2011:\n",
    "    def __init__(self, data, categorical_nodes, gamma_max, sig_level, threshold_for_discretization_dict):\n",
    "        self.graph = nx.DiGraph()\n",
    "        # todo: verify categorical data\n",
    "        # categorical_nodes = list(data.columns)\n",
    "        # for tnode in data.columns:\n",
    "        #     temp_var = []\n",
    "        #     for v in data[tnode]:\n",
    "        #         if v > threshold_for_discretization_dict[tnode][0]:\n",
    "        #             temp_var.append(True)\n",
    "        #         else:\n",
    "        #             temp_var.append(False)\n",
    "        #     print(tnode, sum(temp_var))\n",
    "        #     data[tnode] = temp_var\n",
    "        #     # threshold_for_discretization_dict[tnode] = []\n",
    "        # print(data)\n",
    "\n",
    "\n",
    "        self.qualitative_nodes = categorical_nodes\n",
    "        self.quantitative_nodes = list(set(data.columns) - set(self.qualitative_nodes))\n",
    "\n",
    "        self.gamma_max = gamma_max\n",
    "        self.sig_level = sig_level\n",
    "        self.threshold_for_discretization_dict = threshold_for_discretization_dict\n",
    "        self.test_threshold_for_discretization()\n",
    "\n",
    "        self.nodes_to_tnodes = dict()\n",
    "        self.tnodes_to_thresholded_tnodes = dict()\n",
    "        self.thresholded_tnodes_to_thresholded_nodes = dict()\n",
    "        self.time_table = dict()\n",
    "        self.tnodes = []\n",
    "        for node in data.columns:\n",
    "            self.nodes_to_tnodes[node] = []\n",
    "            for gamma in range(2 * self.gamma_max + 1):\n",
    "                if gamma == 0:\n",
    "                    temporal_node = str(node) + \"_t\"\n",
    "                    self.nodes_to_tnodes[node].append(temporal_node)\n",
    "                    self.tnodes.append(temporal_node)\n",
    "                    self.time_table[temporal_node] = gamma\n",
    "                else:\n",
    "                    temporal_node = str(node) + \"_t_\" + str(gamma)\n",
    "                    self.nodes_to_tnodes[node].append(temporal_node)\n",
    "                    self.tnodes.append(temporal_node)\n",
    "                    self.time_table[temporal_node] = gamma\n",
    "\n",
    "                ###################################################################################\n",
    "                # create thresholded nodes names\n",
    "                if len(threshold_for_discretization_dict[node]) > 1:\n",
    "                    threshold_col_list = self.threshold_for_discretization_dict[node].copy()\n",
    "                    threshold_col_list.sort()\n",
    "                    for th in range(len(threshold_col_list)):\n",
    "                        if th == 0:\n",
    "                            thresholded_tnode = str(temporal_node) + \"<\" + str(round(threshold_col_list[th], ndigits=2))\n",
    "                            thresholded_node = str(node) + \"<\" + str(round(threshold_col_list[th], ndigits=2))\n",
    "                        else:\n",
    "                            thresholded_tnode = str(temporal_node) + \">\" + str(round(threshold_col_list[th], ndigits=2))\n",
    "                            thresholded_node = str(node) + \">\" + str(round(threshold_col_list[th], ndigits=2))\n",
    "                        if temporal_node in self.tnodes_to_thresholded_tnodes.keys():\n",
    "                            self.tnodes_to_thresholded_tnodes[temporal_node].append(thresholded_tnode)\n",
    "                        else:\n",
    "                            self.tnodes_to_thresholded_tnodes[temporal_node] = [thresholded_tnode]\n",
    "                        self.thresholded_tnodes_to_thresholded_nodes[thresholded_tnode] = thresholded_node\n",
    "                else:\n",
    "                    thresholded_tnode = str(temporal_node) + \">\" + str(round(self.threshold_for_discretization_dict[node][0], ndigits=2))\n",
    "                    thresholded_node = str(node) + \">\" + str(round(self.threshold_for_discretization_dict[node][0], ndigits=2))\n",
    "                    if temporal_node in self.tnodes_to_thresholded_tnodes.keys():\n",
    "                        self.tnodes_to_thresholded_tnodes[temporal_node].append(thresholded_tnode)\n",
    "                    else:\n",
    "                        self.tnodes_to_thresholded_tnodes[temporal_node] = [thresholded_tnode]\n",
    "                    self.thresholded_tnodes_to_thresholded_nodes[thresholded_tnode] = thresholded_node\n",
    "                ###################################################################################\n",
    "\n",
    "        # create inverse dicts\n",
    "        ###################################################################################\n",
    "        self.tnodes_to_nodes = {v: k for k, v_list in self.nodes_to_tnodes.items() for v in v_list}\n",
    "        self.thresholded_tnodes_to_tnodes = {v: k for k, v_list in\n",
    "                                                    self.tnodes_to_thresholded_tnodes.items() for v in v_list}\n",
    "        self.tnodes_or_thresholded_tnodes_to_nodes = self.tnodes_to_nodes.copy()\n",
    "        for thresholded_nodes in self.thresholded_tnodes_to_tnodes.keys():\n",
    "            self.tnodes_or_thresholded_tnodes_to_nodes[thresholded_nodes] = \\\n",
    "                self.tnodes_to_nodes[self.thresholded_tnodes_to_tnodes[thresholded_nodes]]\n",
    "        ###################################################################################\n",
    "\n",
    "        print('#####################################')\n",
    "        print(self.tnodes)\n",
    "        print(self.nodes_to_tnodes)\n",
    "        print(self.tnodes_to_nodes)\n",
    "        print(self.tnodes_to_thresholded_tnodes)\n",
    "        print(self.thresholded_tnodes_to_tnodes)\n",
    "        print(self.tnodes_or_thresholded_tnodes_to_nodes)\n",
    "        print('#####################################')\n",
    "\n",
    "        self.normal_nodes = []\n",
    "        self.anomalous_nodes = []\n",
    "        self.data = self._process_data(data)\n",
    "        self.discretized_data = self._quantitative_to_qualitative()\n",
    "\n",
    "        self.prima_facie_causes = dict()\n",
    "        self.genuine_causes = dict()\n",
    "        self.root_causes = []\n",
    "\n",
    "    def test_threshold_for_discretization(self):\n",
    "        for k in self.threshold_for_discretization_dict.keys():\n",
    "            if len(self.threshold_for_discretization_dict[k]) > 2:\n",
    "                print(\"Error: too many thresholds for time series (\" + str(k) + \"). Max thresholds allowed is 2.\")\n",
    "                exit(0)\n",
    "\n",
    "    # Todo: tansform series with mutltiple threshold into many binary variables (with true false values)\n",
    "    def _quantitative_to_qualitative(self):\n",
    "        discretized_variables = []\n",
    "        for tnode in self.data.columns:\n",
    "            if self.tnodes_to_nodes[tnode] in self.quantitative_nodes:\n",
    "                discretized_variables.append(tnode)\n",
    "        discretized_data = pd.DataFrame(columns=discretized_variables)\n",
    "        tnodes_to_eliminate = []\n",
    "        for tnode in discretized_data.columns:\n",
    "            discretized_data_col = []\n",
    "            threshold_col_list = self.threshold_for_discretization_dict[self.tnodes_to_nodes[tnode]].copy()\n",
    "            threshold_col_list.sort()\n",
    "            if len(threshold_col_list) == 1:\n",
    "                for v in self.data[tnode]:\n",
    "                    if v > threshold_col_list[0]:\n",
    "                        discretized_data_col.append(True)\n",
    "                    else:\n",
    "                        discretized_data_col.append(False)\n",
    "\n",
    "                if len(set(discretized_data_col)) == 1:\n",
    "                    self.normal_nodes.append(self.tnodes_to_thresholded_tnodes[tnode][0])\n",
    "                else:\n",
    "                    self.anomalous_nodes.append(self.tnodes_to_thresholded_tnodes[tnode][0])\n",
    "                discretized_data[self.tnodes_to_thresholded_tnodes[tnode][0]] = discretized_data_col\n",
    "            else:\n",
    "                for v in self.data[tnode]:\n",
    "                    #######################################################################################\n",
    "                    # first approach: distinguish between top outliers and down outliers\n",
    "                    one_vs_many = [False] * 2\n",
    "                    if v < threshold_col_list[0]:\n",
    "                        one_vs_many[0] = True\n",
    "                    elif v > threshold_col_list[1]:\n",
    "                        one_vs_many[1] = True\n",
    "                    discretized_data_col.append(one_vs_many)\n",
    "                    #######################################################################################\n",
    "                    # second approach: treat top outliers and down outliers as the same outlier\n",
    "                    # if (v < threshold_col_list[0]) or (v > threshold_col_list[1]):\n",
    "                    #     discretized_data_col.append(True)\n",
    "                    # else:\n",
    "                    #     discretized_data_col.append(False)\n",
    "                    #######################################################################################\n",
    "                discretized_data_col = np.array(discretized_data_col)\n",
    "                for i in range(len(threshold_col_list)):\n",
    "                    if len(set(discretized_data_col[:, i])) != 1:\n",
    "                        discretized_data[self.tnodes_to_thresholded_tnodes[tnode][i]] = \\\n",
    "                            discretized_data_col[:, i]\n",
    "                    if len(set(discretized_data_col[:, i])) == 1:\n",
    "                        self.normal_nodes.append(self.tnodes_to_thresholded_tnodes[tnode][i])\n",
    "                    else:\n",
    "                        self.anomalous_nodes.append(self.tnodes_to_thresholded_tnodes[tnode][i])\n",
    "            if tnode not in tnodes_to_eliminate:\n",
    "                tnodes_to_eliminate.append(tnode)\n",
    "        discretized_data.drop(tnodes_to_eliminate, axis=1, inplace=True)\n",
    "        return discretized_data\n",
    "\n",
    "    def _process_data(self, data):\n",
    "        new_data = pd.DataFrame()\n",
    "        # for gamma in range(0, 2 * self.gamma_max + 1):\n",
    "        #     shifted_data = data.shift(periods=-2 * self.gamma_max + gamma)\n",
    "        for gamma in range(0, self.gamma_max + 1):\n",
    "            shifted_data = data.shift(periods=self.gamma_max + gamma)\n",
    "            new_columns = []\n",
    "            for node in data.columns:\n",
    "                new_columns.append(self.nodes_to_tnodes[node][gamma])\n",
    "            shifted_data.columns = new_columns\n",
    "            new_data = pd.concat([new_data, shifted_data], axis=1, join=\"outer\")\n",
    "        new_data.dropna(axis=0, inplace=True)\n",
    "        return new_data\n",
    "\n",
    "    def is_prima_facie(self, temporal_effect, temporal_or_thresholded_cause):\n",
    "        effect_value = list(self.data[temporal_effect])\n",
    "\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[temporal_or_thresholded_cause] in self.quantitative_nodes:\n",
    "            cause_value = list(self.discretized_data[temporal_or_thresholded_cause])\n",
    "        else:\n",
    "            cause_value = list(self.data[temporal_or_thresholded_cause])\n",
    "\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[temporal_effect] in self.quantitative_nodes:\n",
    "            mean_e = mean(effect_value)\n",
    "            list_e_c = []\n",
    "            for (c, e) in zip(cause_value, effect_value):\n",
    "                if c == True:\n",
    "                    list_e_c.append(e)\n",
    "            # return(stats.ttest_ind(effect_value, list_e_c, permutations=500, equal_var=False)[1] <= 0.05)\n",
    "            return mean(list_e_c) != mean_e\n",
    "        else:\n",
    "            c_and_e = sum([c and e for (c, e) in zip(cause_value, effect_value)])\n",
    "            c_true = sum(cause_value)\n",
    "            e_true = sum(effect_value)\n",
    "            events = len(effect_value)\n",
    "            if c_true == 0:\n",
    "                return False\n",
    "            result = (c_and_e / c_true) != (e_true / events)\n",
    "            return result\n",
    "\n",
    "    def find_prima_facie_causes(self):\n",
    "        nodes_t = [node for node in self.data.columns if self.time_table[node] == 0]\n",
    "        for temporal_effect in nodes_t:\n",
    "            for temporal_cause in self.data.columns:\n",
    "                # condition on temporal priority between cause and effect\n",
    "                if self.time_table[temporal_effect] - self.time_table[temporal_cause] < 0:\n",
    "                    if temporal_effect != temporal_cause:\n",
    "                        # take into account multi thresholding\n",
    "                        if temporal_cause in self.tnodes_to_thresholded_tnodes.keys():\n",
    "                            list_temporal_or_thresholded_causes = \\\n",
    "                                self.tnodes_to_thresholded_tnodes[temporal_cause]\n",
    "                        else:\n",
    "                            list_temporal_or_thresholded_causes = [temporal_cause]\n",
    "                        for temporal_or_thresholded_cause in list_temporal_or_thresholded_causes:\n",
    "                            if temporal_or_thresholded_cause in self.anomalous_nodes:\n",
    "                                if self.is_prima_facie(temporal_effect, temporal_or_thresholded_cause):\n",
    "                                    if temporal_effect in self.prima_facie_causes:\n",
    "                                        self.prima_facie_causes[temporal_effect].append(temporal_or_thresholded_cause)\n",
    "                                    else:\n",
    "                                        self.prima_facie_causes[temporal_effect] = [temporal_or_thresholded_cause]\n",
    "\n",
    "    def get_other_causes(self, temporal_or_thresholded_cause, temporal_effect):\n",
    "        # if temporal_or_thresholded_cause in self.thresholded_tnodes_to_tnodes.keys():\n",
    "        #     temporal_cause = self.thresholded_tnodes_to_tnodes[temporal_or_thresholded_cause]\n",
    "        #     main_cause = self.tnodes_to_thresholded_tnodes[temporal_cause]\n",
    "        # else:\n",
    "        #     main_cause = [temporal_or_thresholded_cause]\n",
    "        main_cause = [temporal_or_thresholded_cause]\n",
    "        other_causes = [cause for cause in self.prima_facie_causes[temporal_effect] if\n",
    "                        cause not in main_cause]\n",
    "        return other_causes\n",
    "\n",
    "    def calculate_probability_difference_2011_no_x(self, temporal_or_thresholded_cause, temporal_effect):\n",
    "        effect_value = list(self.data[temporal_effect])\n",
    "\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[temporal_or_thresholded_cause] in self.quantitative_nodes:\n",
    "            cause_value = list(self.discretized_data[temporal_or_thresholded_cause])\n",
    "        else:\n",
    "            cause_value = list(self.data[temporal_or_thresholded_cause])\n",
    "\n",
    "        if self.tnodes_to_nodes[temporal_effect] in self.qualitative_nodes:\n",
    "            e_and_c = [e and c for (e, c) in zip(effect_value, cause_value)]\n",
    "\n",
    "            e_and_not_c = [e and (not c) for (e, c) in zip(effect_value, cause_value)]\n",
    "\n",
    "            return sum(e_and_c) - sum(e_and_not_c)\n",
    "        else:\n",
    "            list_e_c = []\n",
    "            for (e, cx) in zip(effect_value, cause_value):\n",
    "                if cx == True:\n",
    "                    list_e_c.append(e)\n",
    "\n",
    "            not_c = [(not c) for c in cause_value]\n",
    "            list_e_not_c = []\n",
    "            for (e, ncx) in zip(effect_value, not_c):\n",
    "                if ncx == True:\n",
    "                    list_e_not_c.append(e)\n",
    "            if len(list_e_c) == 0:\n",
    "                mean_e_c = 0\n",
    "            else:\n",
    "                mean_e_c = mean(list_e_c)\n",
    "            if len(list_e_not_c) == 0:\n",
    "                mean_e_not_c = 0\n",
    "            else:\n",
    "                mean_e_not_c = mean(list_e_not_c)\n",
    "            return mean_e_c - mean_e_not_c\n",
    "\n",
    "    def calculate_probability_difference_2011(self, temporal_or_thresholded_cause, temporal_effect, x):\n",
    "        effect_value = list(self.data[temporal_effect])\n",
    "\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[temporal_or_thresholded_cause] in self.quantitative_nodes:\n",
    "            cause_value = list(self.discretized_data[temporal_or_thresholded_cause])\n",
    "        else:\n",
    "            cause_value = list(self.data[temporal_or_thresholded_cause])\n",
    "        if self.tnodes_or_thresholded_tnodes_to_nodes[x] in self.quantitative_nodes:\n",
    "            x_value = list(self.discretized_data[x])\n",
    "        else:\n",
    "            x_value = list(self.data[x])\n",
    "\n",
    "        if self.tnodes_to_nodes[temporal_effect] in self.qualitative_nodes:\n",
    "            c_and_x = [c and x for (c, x) in zip(cause_value, x_value)]\n",
    "            e_and_c_and_x = [e and cx for (e, cx) in zip(effect_value, c_and_x)]\n",
    "\n",
    "            not_c_and_x = [(not c) and x for (c, x) in zip(cause_value, x_value)]\n",
    "            e_and_not_c_and_x = [e and ncx for (e, ncx) in zip(effect_value, not_c_and_x)]\n",
    "\n",
    "            if sum(c_and_x) == 0 or sum(not_c_and_x) == 0:\n",
    "                return None\n",
    "\n",
    "            return sum(e_and_c_and_x) / sum(c_and_x) - sum(e_and_not_c_and_x) / sum(not_c_and_x)\n",
    "        else:\n",
    "            c_and_x = [c and x for (c, x) in zip(cause_value, x_value)]\n",
    "            list_e_c = []\n",
    "            for (e, cx) in zip(effect_value, c_and_x):\n",
    "                if cx == True:\n",
    "                    list_e_c.append(e)\n",
    "\n",
    "            not_c_and_x = [(not c) and x for (c, x) in zip(cause_value, x_value)]\n",
    "            list_e_not_c = []\n",
    "            for (e, ncx) in zip(effect_value, not_c_and_x):\n",
    "                if ncx == True:\n",
    "                    list_e_not_c.append(e)\n",
    "            if len(list_e_c) == 0:\n",
    "                mean_e_c = 0\n",
    "            else:\n",
    "                mean_e_c = mean(list_e_c)\n",
    "            if len(list_e_not_c) == 0:\n",
    "                mean_e_not_c = 0\n",
    "            else:\n",
    "                mean_e_not_c = mean(list_e_not_c)\n",
    "            return mean_e_c - mean_e_not_c\n",
    "\n",
    "    # Auxiliary method\n",
    "    def get_epsilon_average_2011(self, temporal_or_thresholded_cause, temporal_effect):\n",
    "        other_causes = self.get_other_causes(temporal_or_thresholded_cause, temporal_effect)\n",
    "        eps_x = 0\n",
    "        if len(other_causes) != 0:\n",
    "            for x in other_causes:\n",
    "                eps_result = self.calculate_probability_difference_2011(temporal_or_thresholded_cause, temporal_effect,\n",
    "                                                                        x)\n",
    "                if eps_result == None:\n",
    "                    return None\n",
    "                eps_x += eps_result\n",
    "            eps_avg = eps_x / len(other_causes)\n",
    "            return eps_avg\n",
    "        else:\n",
    "            return self.calculate_probability_difference_2011_no_x(temporal_or_thresholded_cause, temporal_effect)\n",
    "\n",
    "    # Main function of logic-based method of 2011\n",
    "    def do_all_epsilon_averages_2011(self):\n",
    "        list_epsilon = {}\n",
    "        for temporal_effect in self.prima_facie_causes:\n",
    "            for temporal_or_thresholded_cause in self.prima_facie_causes[temporal_effect]:\n",
    "                list_epsilon[(temporal_or_thresholded_cause, temporal_effect)] = \\\n",
    "                    self.get_epsilon_average_2011(temporal_or_thresholded_cause, temporal_effect)\n",
    "        return list_epsilon\n",
    "\n",
    "    def find_genuine_causes(self, all_epsilon_averages):\n",
    "        for ce in all_epsilon_averages.keys():\n",
    "            if abs(all_epsilon_averages[ce]) >= self.sig_level:\n",
    "                if ce[1] in self.genuine_causes:\n",
    "                    self.genuine_causes[ce[1]].append(ce[0])\n",
    "                else:\n",
    "                    self.genuine_causes[ce[1]] = [ce[0]]\n",
    "                    \n",
    "    def find_prob_of_root_causes(self, epsilon):\n",
    "        graph = self.construct_summary_graph(plot=False)\n",
    "        prob_root = {}\n",
    "        for child in graph.nodes:\n",
    "            prob_root[child] = {}\n",
    "            prob_root[child][child] = pow(epsilon,len(list(graph.predecessors(child))))\n",
    "            for root in nx.ancestors(graph, child):\n",
    "                all_paths = list(nx.all_simple_paths(graph, source=root, target=child))\n",
    "                prob = 0\n",
    "                for path in all_paths:\n",
    "                    prob += pow(1-epsilon,len(path)-1)\n",
    "                if len(list(graph.predecessors(root))) != 0:\n",
    "                    prob *= pow(epsilon,len(list(graph.predecessors(root))))\n",
    "                prob_root[child][root] = prob\n",
    "            z = sum(prob_root[child].values())\n",
    "            for root in prob_root[child].keys():\n",
    "                prob_root[child][root] = prob_root[child][root]/z\n",
    "        return prob_root\n",
    "    \n",
    "    # todo\n",
    "    def find_root_causes_of_anomalies(self):\n",
    "        summary_graph = self.construct_summary_graph(plot=True)\n",
    "        for node in summary_graph.nodes:\n",
    "            parents_of_node = list(summary_graph.predecessors(node))\n",
    "            if len(parents_of_node) == 0:\n",
    "                self.root_causes.append(node)\n",
    "            else:\n",
    "                if (len(parents_of_node) == 1) and parents_of_node[0] == node:\n",
    "                    self.root_causes.append(node)\n",
    "\n",
    "    def construct_temporal_graph(self, plot=True):\n",
    "        list_nodes_1 = []\n",
    "        list_edges_1 = []\n",
    "\n",
    "        for temporal_node in self.tnodes:\n",
    "            if self.time_table[temporal_node] == 0:\n",
    "                list_nodes_1.append(temporal_node)\n",
    "\n",
    "        for effect in self.genuine_causes.keys():\n",
    "            for cause in self.genuine_causes[effect]:\n",
    "                if cause in self.thresholded_tnodes_to_tnodes.keys():\n",
    "                    cause = self.thresholded_tnodes_to_tnodes[cause]\n",
    "                if cause not in list_nodes_1:\n",
    "                    list_nodes_1.append(cause)\n",
    "                if (cause, effect) not in list_edges_1:\n",
    "                    list_edges_1.append((cause, effect))\n",
    "\n",
    "        temporal_graph = nx.DiGraph()\n",
    "        temporal_graph.add_nodes_from(list_nodes_1)\n",
    "        temporal_graph.add_edges_from(list_edges_1)\n",
    "        pos = dict()\n",
    "        all_nodes = self.qualitative_nodes + self.quantitative_nodes\n",
    "        for temporal_node in list_nodes_1:\n",
    "            node = self.tnodes_to_nodes[temporal_node]\n",
    "            y = all_nodes.index(node)\n",
    "            pos[temporal_node] = [-self.time_table[temporal_node], y]\n",
    "\n",
    "        if plot:\n",
    "            nx.draw(temporal_graph, pos, with_labels=True)\n",
    "            plt.show()\n",
    "        return temporal_graph\n",
    "\n",
    "    def construct_summary_graph(self, plot=True):\n",
    "        list_nodes_1 = []\n",
    "        list_edges_1 = []\n",
    "        for node in self.qualitative_nodes + self.quantitative_nodes:\n",
    "            list_nodes_1.append(node)\n",
    "\n",
    "        for effect_t in self.genuine_causes.keys():\n",
    "            for cause_t in self.genuine_causes[effect_t]:\n",
    "                effect = self.tnodes_to_nodes[effect_t]\n",
    "                cause = self.tnodes_or_thresholded_tnodes_to_nodes[cause_t]\n",
    "                if (cause, effect) not in list_edges_1:\n",
    "                    list_edges_1.append((cause, effect))\n",
    "\n",
    "        summary_graph = nx.DiGraph()\n",
    "        summary_graph.add_nodes_from(list_nodes_1)\n",
    "        summary_graph.add_edges_from(list_edges_1)\n",
    "        if plot:\n",
    "            nx.draw(summary_graph, with_labels=True)\n",
    "            plt.show()\n",
    "        return summary_graph\n",
    "\n",
    "    def construct_temporal_outlier_graph(self, plot=True):\n",
    "        # todo adapt to multi threshold\n",
    "        list_nodes_0 = []\n",
    "        # list_nodes_1 = []\n",
    "        list_edges_1 = []\n",
    "        for temporal_node in self.tnodes:\n",
    "            if self.time_table[temporal_node] == 0:\n",
    "                list_nodes_0.append(temporal_node)\n",
    "\n",
    "        for effect_t in self.genuine_causes.keys():\n",
    "            for cause_tt in self.genuine_causes[effect_t]:\n",
    "                list_nodes_0.append(cause_tt)\n",
    "                list_edges_1.append((cause_tt, effect_t))\n",
    "\n",
    "        temporal_outlier_graph = nx.DiGraph()\n",
    "        temporal_outlier_graph.add_nodes_from(list_nodes_0)\n",
    "        temporal_outlier_graph.add_edges_from(list_edges_1)\n",
    "        pos = dict()\n",
    "        all_nodes = []\n",
    "        all_thresholded_nodes = dict()\n",
    "        for t in range(1, self.gamma_max + 1):\n",
    "            all_thresholded_nodes[t] = []\n",
    "        for node in self.qualitative_nodes + self.quantitative_nodes:\n",
    "            for node_t_ in self.nodes_to_tnodes[node]:\n",
    "                if 0 < self.time_table[node_t_] <= self.gamma_max:\n",
    "                    if node_t_ in self.tnodes_to_thresholded_tnodes.keys():\n",
    "                        all_thresholded_nodes[self.time_table[node_t_]] = \\\n",
    "                            all_thresholded_nodes[self.time_table[node_t_]] + \\\n",
    "                            self.tnodes_to_thresholded_tnodes[node_t_] + [None]\n",
    "                        if node not in all_nodes:\n",
    "                            all_nodes = all_nodes + [node] * len(self.tnodes_to_thresholded_tnodes[node_t_]) + [None]\n",
    "                    else:\n",
    "                        all_thresholded_nodes[self.time_table[node_t_]] = \\\n",
    "                            all_thresholded_nodes[self.time_table[node_t_]] + [node_t_] + [None]\n",
    "                        if node not in all_nodes:\n",
    "                            all_nodes = all_nodes + [node] + [None]\n",
    "        for node_tt in list_nodes_0:\n",
    "            node = self.tnodes_or_thresholded_tnodes_to_nodes[node_tt]\n",
    "            if node_tt in self.thresholded_tnodes_to_tnodes.keys():\n",
    "                node_t_ = self.thresholded_tnodes_to_tnodes[node_tt]\n",
    "            else:\n",
    "                node_t_ = node_tt\n",
    "            if self.time_table[node_t_] > 0:\n",
    "                y = all_thresholded_nodes[self.time_table[node_t_]].index(node_tt)\n",
    "            else:\n",
    "                y = all_nodes.index(node)\n",
    "            pos[node_tt] = [-self.time_table[node_t_], y]\n",
    "        if plot:\n",
    "            nx.draw(temporal_outlier_graph, pos, with_labels=True)\n",
    "            plt.show()\n",
    "        return temporal_outlier_graph\n",
    "\n",
    "    def construct_outlier_graph(self, plot=True):\n",
    "        list_effects = []\n",
    "        list_causes = []\n",
    "        list_edges_1 = []\n",
    "        for node in self.qualitative_nodes + self.quantitative_nodes:\n",
    "            list_effects.append(node)\n",
    "\n",
    "        processed_causes_to_causes = dict()\n",
    "        for teffect in self.genuine_causes.keys():\n",
    "            effect = self.tnodes_to_nodes[teffect]\n",
    "            for tnodes_or_thresholded_tnodes in self.genuine_causes[teffect]:\n",
    "                cause = self.tnodes_or_thresholded_tnodes_to_nodes[tnodes_or_thresholded_tnodes]\n",
    "                if len(self.threshold_for_discretization_dict[cause]) > 0:\n",
    "                    processed_cause = self.thresholded_tnodes_to_thresholded_nodes[tnodes_or_thresholded_tnodes]\n",
    "                else:\n",
    "                    processed_cause = cause + \"--\"\n",
    "                processed_causes_to_causes[processed_cause] = cause\n",
    "                if processed_cause not in list_causes:\n",
    "                    list_causes.append(processed_cause)\n",
    "                if (processed_cause, effect) not in list_edges_1:\n",
    "                    list_edges_1.append((processed_cause, effect))\n",
    "\n",
    "        outlier_graph = nx.DiGraph()\n",
    "        outlier_graph.add_nodes_from(list_effects)\n",
    "        outlier_graph.add_nodes_from(list_causes)\n",
    "        outlier_graph.add_edges_from(list_edges_1)\n",
    "\n",
    "        pos = dict()\n",
    "        for effect in list_effects:\n",
    "            # cause = processed_causes_to_causes[processed_cause]\n",
    "            y = list_effects.index(effect)\n",
    "            pos[effect] = [1, y]\n",
    "        for processed_cause in list_causes:\n",
    "            y = list_causes.index(processed_cause)\n",
    "            pos[processed_cause] = [0, y]\n",
    "\n",
    "        if plot:\n",
    "            nx.draw(outlier_graph, pos, with_labels=True)\n",
    "            # nx.draw(G, with_labels=True, pos=nx.drawing.layout.bipartite_layout(G, list_nodes_1),)\n",
    "            plt.show()\n",
    "        return outlier_graph\n",
    "\n",
    "\n",
    "# # Auxiliary method\n",
    "# def calculate_probability_difference_2015(cause, effect, other_causes, data, boolean_variables, threshold_dict, dict_anomaly, ratio_normal, ratio_anomaly):\n",
    "#     cause_value = list(data[cause])\n",
    "#     effect_value = list(data[effect])\n",
    "#     x_value = np.zeros(shape=(len(effect_value), len(other_causes)), dtype=bool)\n",
    "#\n",
    "#     index = 0\n",
    "#     for x in other_causes:\n",
    "#         if x not in boolean_variables:\n",
    "#             x_value[:, index] = continue_to_boolean(list(data[x]), threshold_dict[x], dict_anomaly[x], ratio_normal, ratio_anomaly)\n",
    "#         else:\n",
    "#             x_value[:, index] = list(data[x])\n",
    "#         index +=1\n",
    "#     if cause not in boolean_variables:\n",
    "#         cause_value = continue_to_boolean(cause_value, threshold_dict[cause], dict_anomaly[cause], ratio_normal, ratio_anomaly)\n",
    "#     if effect in boolean_variables:\n",
    "#         c_and_x = []\n",
    "#         for i in range(len(effect_value)):\n",
    "#             if cause_value[i] == True and sum(x_value[i, ]) == 0:\n",
    "#                 c_and_x.append(True)\n",
    "#             else:\n",
    "#                 c_and_x.append(False)\n",
    "#         e_and_c_and_x = [e and cx for (e, cx) in zip(effect_value, c_and_x)]\n",
    "#\n",
    "#         not_c_and_x = []\n",
    "#         for i in range(len(effect_value)):\n",
    "#             if cause_value[i]==False and sum(x_value[i, ]) == 0:\n",
    "#                 not_c_and_x.append(True)\n",
    "#             else:\n",
    "#                 not_c_and_x.append(False)\n",
    "#         e_and_not_c_and_x = [e and ncx for (e, ncx) in zip(effect_value, not_c_and_x)]\n",
    "#\n",
    "#         if sum(c_and_x) == 0 or sum(not_c_and_x) == 0:\n",
    "#             return None\n",
    "#\n",
    "#         return sum(e_and_c_and_x)/sum(c_and_x) - sum(e_and_not_c_and_x)/sum(not_c_and_x)\n",
    "#     else:\n",
    "#         list_e_c = []\n",
    "#         list_e_not_c = []\n",
    "#         for i in range(len(effect_value)):\n",
    "#             if cause_value[i]==True and sum(x_value[i,])==0:\n",
    "#                 list_e_c.append(effect_value[i])\n",
    "#             if cause_value[i]==False and sum(x_value[i,])==0:\n",
    "#                 list_e_not_c.append(effect_value[i])\n",
    "#         if len(list_e_c) == 0:\n",
    "#             mean_e_c = 0\n",
    "#         else:\n",
    "#             mean_e_c = mean(list_e_c)\n",
    "#         if len(list_e_not_c) == 0:\n",
    "#             mean_e_not_c = 0\n",
    "#         else:\n",
    "#             mean_e_not_c = mean(list_e_not_c)\n",
    "#         return mean_e_c - mean_e_not_c\n",
    "#\n",
    "#\n",
    "# # Auxiliary method\n",
    "# def get_epsilon_average_2015(cause, effect, relations, data, boolean_variables, threshold_dict, dict_anomaly, ratio_normal, ratio_anomaly):\n",
    "#     other_causes = get_other_causes(cause, effect, relations)\n",
    "#     eps = calculate_probability_difference_2015(cause, effect, other_causes, data, boolean_variables, threshold_dict, dict_anomaly, ratio_normal, ratio_anomaly)\n",
    "#     return eps\n",
    "#\n",
    "#\n",
    "# # Main function of logic-based method of 2015\n",
    "# def do_all_epsilon_averages_2015(relations, data, boolean_variables, threshold_dict, dict_anomaly, ratio_normal, ratio_anomaly):\n",
    "#     list_epsilon = {}\n",
    "#     for effect in relations:\n",
    "#         for cause in relations[effect]:\n",
    "#             list_epsilon[(cause, effect)] = get_epsilon_average_2015(cause, effect, relations, data, boolean_variables,\n",
    "#                                                                      threshold_dict, dict_anomaly, ratio_normal, ratio_anomaly)\n",
    "#     return list_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068fa401-b9ba-4dd9-bf48-f19b64d145df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder_path = os.path.join('..', 'RCA_simulated_data', 'certain', 'data')\n",
    "data_files = [os.path.join(data_folder_path, f) for f in os.listdir(data_folder_path) if os.path.isfile(os.path.join(data_folder_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c820e02f-b72c-46e5-ae5c-b06aa9a2b59f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = data_files[0]\n",
    "param_data = pd.read_csv(data_path)\n",
    "\n",
    "graph_path = os.path.join('..', 'RCA_simulated_data', 'certain', 'data_info', data_path.split('/')[-1].replace('data', 'info').replace('csv', 'json'))\n",
    "with open(graph_path, 'r') as json_file:\n",
    "    json_graph = json.load(json_file)\n",
    "param_threshold_dict = json_graph['nodes_thres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041186e9-107a-4c5f-a1ce-5cc269023d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "['a_t', 'a_t_1', 'a_t_2', 'a_t_3', 'a_t_4', 'a_t_5', 'a_t_6', 'b_t', 'b_t_1', 'b_t_2', 'b_t_3', 'b_t_4', 'b_t_5', 'b_t_6', 'd_t', 'd_t_1', 'd_t_2', 'd_t_3', 'd_t_4', 'd_t_5', 'd_t_6', 'c_t', 'c_t_1', 'c_t_2', 'c_t_3', 'c_t_4', 'c_t_5', 'c_t_6', 'e_t', 'e_t_1', 'e_t_2', 'e_t_3', 'e_t_4', 'e_t_5', 'e_t_6', 'f_t', 'f_t_1', 'f_t_2', 'f_t_3', 'f_t_4', 'f_t_5', 'f_t_6']\n",
      "{'a': ['a_t', 'a_t_1', 'a_t_2', 'a_t_3', 'a_t_4', 'a_t_5', 'a_t_6'], 'b': ['b_t', 'b_t_1', 'b_t_2', 'b_t_3', 'b_t_4', 'b_t_5', 'b_t_6'], 'd': ['d_t', 'd_t_1', 'd_t_2', 'd_t_3', 'd_t_4', 'd_t_5', 'd_t_6'], 'c': ['c_t', 'c_t_1', 'c_t_2', 'c_t_3', 'c_t_4', 'c_t_5', 'c_t_6'], 'e': ['e_t', 'e_t_1', 'e_t_2', 'e_t_3', 'e_t_4', 'e_t_5', 'e_t_6'], 'f': ['f_t', 'f_t_1', 'f_t_2', 'f_t_3', 'f_t_4', 'f_t_5', 'f_t_6']}\n",
      "{'a_t': 'a', 'a_t_1': 'a', 'a_t_2': 'a', 'a_t_3': 'a', 'a_t_4': 'a', 'a_t_5': 'a', 'a_t_6': 'a', 'b_t': 'b', 'b_t_1': 'b', 'b_t_2': 'b', 'b_t_3': 'b', 'b_t_4': 'b', 'b_t_5': 'b', 'b_t_6': 'b', 'd_t': 'd', 'd_t_1': 'd', 'd_t_2': 'd', 'd_t_3': 'd', 'd_t_4': 'd', 'd_t_5': 'd', 'd_t_6': 'd', 'c_t': 'c', 'c_t_1': 'c', 'c_t_2': 'c', 'c_t_3': 'c', 'c_t_4': 'c', 'c_t_5': 'c', 'c_t_6': 'c', 'e_t': 'e', 'e_t_1': 'e', 'e_t_2': 'e', 'e_t_3': 'e', 'e_t_4': 'e', 'e_t_5': 'e', 'e_t_6': 'e', 'f_t': 'f', 'f_t_1': 'f', 'f_t_2': 'f', 'f_t_3': 'f', 'f_t_4': 'f', 'f_t_5': 'f', 'f_t_6': 'f'}\n",
      "{'a_t': ['a_t>0.87'], 'a_t_1': ['a_t_1>0.87'], 'a_t_2': ['a_t_2>0.87'], 'a_t_3': ['a_t_3>0.87'], 'a_t_4': ['a_t_4>0.87'], 'a_t_5': ['a_t_5>0.87'], 'a_t_6': ['a_t_6>0.87'], 'b_t': ['b_t>0.82'], 'b_t_1': ['b_t_1>0.82'], 'b_t_2': ['b_t_2>0.82'], 'b_t_3': ['b_t_3>0.82'], 'b_t_4': ['b_t_4>0.82'], 'b_t_5': ['b_t_5>0.82'], 'b_t_6': ['b_t_6>0.82'], 'd_t': ['d_t>0.73'], 'd_t_1': ['d_t_1>0.73'], 'd_t_2': ['d_t_2>0.73'], 'd_t_3': ['d_t_3>0.73'], 'd_t_4': ['d_t_4>0.73'], 'd_t_5': ['d_t_5>0.73'], 'd_t_6': ['d_t_6>0.73'], 'c_t': ['c_t>0.88'], 'c_t_1': ['c_t_1>0.88'], 'c_t_2': ['c_t_2>0.88'], 'c_t_3': ['c_t_3>0.88'], 'c_t_4': ['c_t_4>0.88'], 'c_t_5': ['c_t_5>0.88'], 'c_t_6': ['c_t_6>0.88'], 'e_t': ['e_t>0.77'], 'e_t_1': ['e_t_1>0.77'], 'e_t_2': ['e_t_2>0.77'], 'e_t_3': ['e_t_3>0.77'], 'e_t_4': ['e_t_4>0.77'], 'e_t_5': ['e_t_5>0.77'], 'e_t_6': ['e_t_6>0.77'], 'f_t': ['f_t>0.72'], 'f_t_1': ['f_t_1>0.72'], 'f_t_2': ['f_t_2>0.72'], 'f_t_3': ['f_t_3>0.72'], 'f_t_4': ['f_t_4>0.72'], 'f_t_5': ['f_t_5>0.72'], 'f_t_6': ['f_t_6>0.72']}\n",
      "{'a_t>0.87': 'a_t', 'a_t_1>0.87': 'a_t_1', 'a_t_2>0.87': 'a_t_2', 'a_t_3>0.87': 'a_t_3', 'a_t_4>0.87': 'a_t_4', 'a_t_5>0.87': 'a_t_5', 'a_t_6>0.87': 'a_t_6', 'b_t>0.82': 'b_t', 'b_t_1>0.82': 'b_t_1', 'b_t_2>0.82': 'b_t_2', 'b_t_3>0.82': 'b_t_3', 'b_t_4>0.82': 'b_t_4', 'b_t_5>0.82': 'b_t_5', 'b_t_6>0.82': 'b_t_6', 'd_t>0.73': 'd_t', 'd_t_1>0.73': 'd_t_1', 'd_t_2>0.73': 'd_t_2', 'd_t_3>0.73': 'd_t_3', 'd_t_4>0.73': 'd_t_4', 'd_t_5>0.73': 'd_t_5', 'd_t_6>0.73': 'd_t_6', 'c_t>0.88': 'c_t', 'c_t_1>0.88': 'c_t_1', 'c_t_2>0.88': 'c_t_2', 'c_t_3>0.88': 'c_t_3', 'c_t_4>0.88': 'c_t_4', 'c_t_5>0.88': 'c_t_5', 'c_t_6>0.88': 'c_t_6', 'e_t>0.77': 'e_t', 'e_t_1>0.77': 'e_t_1', 'e_t_2>0.77': 'e_t_2', 'e_t_3>0.77': 'e_t_3', 'e_t_4>0.77': 'e_t_4', 'e_t_5>0.77': 'e_t_5', 'e_t_6>0.77': 'e_t_6', 'f_t>0.72': 'f_t', 'f_t_1>0.72': 'f_t_1', 'f_t_2>0.72': 'f_t_2', 'f_t_3>0.72': 'f_t_3', 'f_t_4>0.72': 'f_t_4', 'f_t_5>0.72': 'f_t_5', 'f_t_6>0.72': 'f_t_6'}\n",
      "{'a_t': 'a', 'a_t_1': 'a', 'a_t_2': 'a', 'a_t_3': 'a', 'a_t_4': 'a', 'a_t_5': 'a', 'a_t_6': 'a', 'b_t': 'b', 'b_t_1': 'b', 'b_t_2': 'b', 'b_t_3': 'b', 'b_t_4': 'b', 'b_t_5': 'b', 'b_t_6': 'b', 'd_t': 'd', 'd_t_1': 'd', 'd_t_2': 'd', 'd_t_3': 'd', 'd_t_4': 'd', 'd_t_5': 'd', 'd_t_6': 'd', 'c_t': 'c', 'c_t_1': 'c', 'c_t_2': 'c', 'c_t_3': 'c', 'c_t_4': 'c', 'c_t_5': 'c', 'c_t_6': 'c', 'e_t': 'e', 'e_t_1': 'e', 'e_t_2': 'e', 'e_t_3': 'e', 'e_t_4': 'e', 'e_t_5': 'e', 'e_t_6': 'e', 'f_t': 'f', 'f_t_1': 'f', 'f_t_2': 'f', 'f_t_3': 'f', 'f_t_4': 'f', 'f_t_5': 'f', 'f_t_6': 'f', 'a_t>0.87': 'a', 'a_t_1>0.87': 'a', 'a_t_2>0.87': 'a', 'a_t_3>0.87': 'a', 'a_t_4>0.87': 'a', 'a_t_5>0.87': 'a', 'a_t_6>0.87': 'a', 'b_t>0.82': 'b', 'b_t_1>0.82': 'b', 'b_t_2>0.82': 'b', 'b_t_3>0.82': 'b', 'b_t_4>0.82': 'b', 'b_t_5>0.82': 'b', 'b_t_6>0.82': 'b', 'd_t>0.73': 'd', 'd_t_1>0.73': 'd', 'd_t_2>0.73': 'd', 'd_t_3>0.73': 'd', 'd_t_4>0.73': 'd', 'd_t_5>0.73': 'd', 'd_t_6>0.73': 'd', 'c_t>0.88': 'c', 'c_t_1>0.88': 'c', 'c_t_2>0.88': 'c', 'c_t_3>0.88': 'c', 'c_t_4>0.88': 'c', 'c_t_5>0.88': 'c', 'c_t_6>0.88': 'c', 'e_t>0.77': 'e', 'e_t_1>0.77': 'e', 'e_t_2>0.77': 'e', 'e_t_3>0.77': 'e', 'e_t_4>0.77': 'e', 'e_t_5>0.77': 'e', 'e_t_6>0.77': 'e', 'f_t>0.72': 'f', 'f_t_1>0.72': 'f', 'f_t_2>0.72': 'f', 'f_t_3>0.72': 'f', 'f_t_4>0.72': 'f', 'f_t_5>0.72': 'f', 'f_t_6>0.72': 'f'}\n",
      "#####################################\n",
      "ai.genuine_causes\n",
      "{'a_t': ['a_t_1>0.87', 'b_t_1>0.82', 'e_t_1>0.77', 'f_t_1>0.72', 'a_t_2>0.87', 'c_t_2>0.88', 'd_t_3>0.73', 'c_t_3>0.88', 'e_t_3>0.77', 'f_t_3>0.72'], 'b_t': ['a_t_1>0.87', 'd_t_1>0.73', 'c_t_1>0.88', 'b_t_2>0.82', 'a_t_3>0.87'], 'd_t': ['a_t_1>0.87', 'b_t_1>0.82', 'a_t_2>0.87'], 'c_t': ['a_t_1>0.87', 'e_t_1>0.77', 'f_t_1>0.72', 'c_t_3>0.88', 'e_t_3>0.77', 'f_t_3>0.72'], 'e_t': ['d_t_1>0.73', 'b_t_2>0.82', 'c_t_2>0.88', 'a_t_3>0.87', 'd_t_3>0.73'], 'f_t': ['d_t_1>0.73', 'b_t_2>0.82', 'c_t_2>0.88', 'a_t_3>0.87', 'd_t_3>0.73']}\n"
     ]
    }
   ],
   "source": [
    "gamma_max=3 \n",
    "sig_level=0.05\n",
    "categorical_nodes=[]\n",
    "\n",
    "ai = RAITIA2011(data=param_data, categorical_nodes=categorical_nodes, gamma_max=gamma_max, sig_level=sig_level, threshold_for_discretization_dict= param_threshold_dict)\n",
    "ai.find_prima_facie_causes()\n",
    "# print(ai.prima_facie_causes)\n",
    "res_1 = ai.do_all_epsilon_averages_2011()\n",
    "# print(res_1)\n",
    "ai.find_genuine_causes(res_1)\n",
    "print('ai.genuine_causes')\n",
    "print(ai.genuine_causes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2f53dc-7f1d-4fe6-9138-15c986ca7463",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': {'f': 0.010524368340661202,\n",
       "  'd': 0.8069220306666557,\n",
       "  'b': 0.0622626893217687,\n",
       "  'c': 0.06049385873475377,\n",
       "  'e': 0.05489563148330587,\n",
       "  'a': 0.004901421452854742},\n",
       " 'c': {'c': 0.011855788028121149,\n",
       "  'f': 0.027853277045986138,\n",
       "  'd': 0.8515195808535262,\n",
       "  'b': 0.07663676227681736,\n",
       "  'e': 0.027853277045986138,\n",
       "  'a': 0.004281314749562995},\n",
       " 'e': {'e': 0.010524368340661203,\n",
       "  'f': 0.054895631483305875,\n",
       "  'd': 0.8069220306666557,\n",
       "  'b': 0.06226268932176871,\n",
       "  'c': 0.060493858734753776,\n",
       "  'a': 0.004901421452854743},\n",
       " 'a': {'a': 0.0011766508542223723,\n",
       "  'f': 0.03300204946431008,\n",
       "  'd': 0.8214591830277789,\n",
       "  'b': 0.06325858026876796,\n",
       "  'c': 0.04810148692061058,\n",
       "  'e': 0.03300204946431008},\n",
       " 'b': {'b': 0.015188229192579124,\n",
       "  'f': 0.046558605296099745,\n",
       "  'd': 0.8468079055650992,\n",
       "  'c': 0.04099606823660957,\n",
       "  'e': 0.046558605296099745,\n",
       "  'a': 0.003890586413512682},\n",
       " 'd': {'d': 0.4776043396887888,\n",
       "  'f': 0.12824865755449802,\n",
       "  'b': 0.11572859411259422,\n",
       "  'c': 0.14238149533066294,\n",
       "  'e': 0.12824865755449802,\n",
       "  'a': 0.007788255758957903}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai.find_prob_of_root_causes(epsilon=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d9ccd2f-2792-45dc-a833-b1cb054d8dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(all_paths_bfs(graph= outlier_graph, source='a', target='d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "61a3f26a-2541-4fc6-a74f-6ac929dfd5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def all_paths_bfs(graph, source, target):\n",
    "    queue = [[(source , source)]]\n",
    "    all_paths = []\n",
    "\n",
    "    while queue:\n",
    "        path = queue.pop(0)\n",
    "        node = path[-1][-1]\n",
    "\n",
    "        if node == target:\n",
    "            all_paths.append(path)\n",
    "            # print(path)\n",
    "        for parent in graph.predecessors(node):\n",
    "            if (node, parent) not in path:\n",
    "                new_path = path + [(node, parent)]\n",
    "                queue.append(new_path)\n",
    "\n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3d773a-34b3-471f-891b-b748d5c1ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = {('a_t_1>0.71', 'a_t'): -0.004546733644733771, ('b_t_1>0.84', 'a_t'): 0.06341369571809329, ('d_t_1>0.75', 'a_t'): 0.04844015973731053, ('c_t_1>0.86', 'a_t'): 0.04086525602712438, ('e_t_1>0.89', 'a_t'): 0.047969656365870625, ('f_t_1>0.87', 'a_t'): 0.034639554233269886, ('a_t_2>0.71', 'a_t'): 0.00784692191961427, ('b_t_2>0.84', 'a_t'): 0.005733394755883873, ('d_t_2>0.75', 'a_t'): 0.010164241265466355, ('c_t_2>0.86', 'a_t'): 0.005800423866349325, ('e_t_2>0.89', 'a_t'): 0.012875782362273198, ('f_t_2>0.87', 'a_t'): -0.0005715609616708553, ('a_t_1>0.71', 'b_t'): 0.33965029168944344, ('b_t_1>0.84', 'b_t'): 0.0585631146771368, ('d_t_1>0.75', 'b_t'): 0.04865701729397205, ('c_t_1>0.86', 'b_t'): 0.04954143122360713, ('e_t_1>0.89', 'b_t'): 0.07389210890606981, ('f_t_1>0.87', 'b_t'): 0.06755483447694528, ('a_t_2>0.71', 'b_t'): -0.005343918941600104, ('b_t_2>0.84', 'b_t'): -0.008473462930151054, ('d_t_2>0.75', 'b_t'): -0.0003388628093858668, ('c_t_2>0.86', 'b_t'): 0.012269617538244515, ('e_t_2>0.89', 'b_t'): -0.016055044130775905, ('f_t_2>0.87', 'b_t'): 0.00019045368843364295, ('a_t_1>0.71', 'd_t'): 0.3483623300600486, ('b_t_1>0.84', 'd_t'): 0.039864284700972, ('d_t_1>0.75', 'd_t'): 0.03905897184238378, ('c_t_1>0.86', 'd_t'): 0.05521941225148744, ('e_t_1>0.89', 'd_t'): 0.05618546535012781, ('f_t_1>0.87', 'd_t'): 0.04431336323115469, ('a_t_2>0.71', 'd_t'): -0.03462267576621796, ('b_t_2>0.84', 'd_t'): 0.01434210674339409, ('d_t_2>0.75', 'd_t'): -0.01305624002935985, ('c_t_2>0.86', 'd_t'): -0.0030793427467373387, ('e_t_2>0.89', 'd_t'): -0.017144499616131435, ('f_t_2>0.87', 'd_t'): -0.002668719047223494, ('a_t_1>0.71', 'c_t'): -0.01132232836789721, ('b_t_1>0.84', 'c_t'): 0.4024627716738744, ('d_t_1>0.75', 'c_t'): 0.19422759727305794, ('c_t_1>0.86', 'c_t'): 0.07560630244480526, ('e_t_1>0.89', 'c_t'): 0.04498256206247147, ('f_t_1>0.87', 'c_t'): 0.04728157180152623, ('a_t_2>0.71', 'c_t'): 0.21722671235178542, ('b_t_2>0.84', 'c_t'): 0.012756513255488584, ('d_t_2>0.75', 'c_t'): -0.005826736797818744, ('c_t_2>0.86', 'c_t'): -0.02256166469232169, ('e_t_2>0.89', 'c_t'): -0.00263152259931889, ('f_t_2>0.87', 'c_t'): 0.001463814157247662, ('a_t_1>0.71', 'e_t'): 0.007414028773731803, ('b_t_1>0.84', 'e_t'): 0.07896051063836369, ('d_t_1>0.75', 'e_t'): 0.0670297880844045, ('c_t_1>0.86', 'e_t'): 0.39039380177983196, ('e_t_1>0.89', 'e_t'): 0.08677519614854225, ('f_t_1>0.87', 'e_t'): 0.07465474196603157, ('a_t_2>0.71', 'e_t'): 0.0012673419396935983, ('b_t_2>0.84', 'e_t'): 0.21675178075369886, ('d_t_2>0.75', 'e_t'): 0.08920871841841327, ('c_t_2>0.86', 'e_t'): 0.006711766960995191, ('e_t_2>0.89', 'e_t'): -0.006932562293166794, ('f_t_2>0.87', 'e_t'): -0.023699107269329726, ('a_t_1>0.71', 'f_t'): -0.00653314444361949, ('b_t_1>0.84', 'f_t'): 0.06460212435625674, ('d_t_1>0.75', 'f_t'): 0.0654200785228019, ('c_t_1>0.86', 'f_t'): 0.41424034528941756, ('e_t_1>0.89', 'f_t'): 0.08535192750322254, ('f_t_1>0.87', 'f_t'): 0.08292201074740381, ('a_t_2>0.71', 'f_t'): -0.01578013826837644, ('b_t_2>0.84', 'f_t'): 0.23469905949327574, ('d_t_2>0.75', 'f_t'): 0.0884603472506603, ('c_t_2>0.86', 'f_t'): 0.01166193246163507, ('e_t_2>0.89', 'f_t'): -0.027883329990265236, ('f_t_2>0.87', 'f_t'): -0.027381300840862675}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537d05d4-e5f6-41a5-968f-ba94a375f91f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa5d5d5d-0e02-41ab-aeee-7d86fe9ae2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "genuine_causes = {}\n",
    "effect_causes_epsilons = {}\n",
    "for ce in res_1.keys():\n",
    "    if ce[1] not in effect_causes_epsilons.keys():\n",
    "        effect_causes_epsilons[ce[1]] = {'causes':[],\n",
    "                                         'epsilons':[]}\n",
    "    effect_causes_epsilons[ce[1]]['causes'].append(ce[0])\n",
    "    effect_causes_epsilons[ce[1]]['epsilons'].append(res_1[ce])\n",
    "    \n",
    "#select genuine causes based on fdr \n",
    "for effect in effect_causes_epsilons.keys():\n",
    "    epsilons = np.array(effect_causes_epsilons[effect]['epsilons'])\n",
    "    z_score = (epsilons-np.mean(epsilons))/np.std(epsilons)\n",
    "    p_values = stats.norm.sf(abs(z_score)) \n",
    "    res, p_values_adapt = fdrcorrection(p_values, alpha=0.1)\n",
    "    if True in res:\n",
    "        genuine_causes[effect] = [effect_causes_epsilons[effect]['causes'][i] for i in np.where(res==True)[0]]\n",
    "    else:\n",
    "        genuine_causes[effect] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47548dca-a0be-4978-8977-0479aba660c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'causes': ['a_t_1>0.71',\n",
       "  'b_t_1>0.84',\n",
       "  'd_t_1>0.75',\n",
       "  'c_t_1>0.86',\n",
       "  'e_t_1>0.89',\n",
       "  'f_t_1>0.87',\n",
       "  'a_t_2>0.71',\n",
       "  'b_t_2>0.84',\n",
       "  'd_t_2>0.75',\n",
       "  'c_t_2>0.86',\n",
       "  'e_t_2>0.89',\n",
       "  'f_t_2>0.87'],\n",
       " 'epsilons': [-0.004546733644733771,\n",
       "  0.06341369571809329,\n",
       "  0.04844015973731053,\n",
       "  0.04086525602712438,\n",
       "  0.047969656365870625,\n",
       "  0.034639554233269886,\n",
       "  0.00784692191961427,\n",
       "  0.005733394755883873,\n",
       "  0.010164241265466355,\n",
       "  0.005800423866349325,\n",
       "  0.012875782362273198,\n",
       "  -0.0005715609616708553]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_causes_epsilons['a_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab5b39a4-31b3-4e61-9867-15315118d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23829086, 0.4474444 , 0.45008162, 0.99670742, 0.51474636,\n",
       "       0.50684965, 0.21559232, 0.89508281, 0.52483922, 0.28653459,\n",
       "       0.18786358, 0.18896805])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_distribution = stats.norm(loc=0,scale=1.) #loc is the mean, scale is the variance.\n",
    "normal_distribution.cdf(z_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80b7bb3a-d0fc-4d20-8020-6dfb8290d80c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81213642, 0.81213642, 0.81213642, 0.03951093, 0.81213642,\n",
       "       0.81213642, 0.81213642, 0.62950314, 0.81213642, 0.81213642,\n",
       "       0.81213642, 0.81213642])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_values_adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cc5c9299-7070-4244-96cd-9801720a3209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.71181116, -0.13212074, -0.12545513,  2.71712593,  0.03697206,\n",
       "        0.01717036, -0.7871661 ,  1.25402098,  0.06230297, -0.56353715,\n",
       "       -0.88579656, -0.88170546])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dad34cc5-ca18-4522-b6da-29884de13be8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76170914, 0.5525556 , 0.54991838, 0.00329258, 0.48525364,\n",
       "       0.49315035, 0.78440768, 0.10491719, 0.47516078, 0.71346541,\n",
       "       0.81213642, 0.81103195])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e13a36-2c16-44d1-9552-cd14a82aad89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
