{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f315073-7faf-4e08-8c15-600d955c54ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d30b46-c020-484a-8bd8-3f1a8088a81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dict_to_graph(graph_dict, inter_nodes):\n",
    "    # Create an empty directed graph\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # Iterate through the dictionary and add nodes and edges to the graph\n",
    "    for parent, children in graph_dict.items():\n",
    "        # Add the parent node to the graph\n",
    "        graph.add_node(parent)\n",
    "\n",
    "        # Iterate through the children of the parent\n",
    "        for child in children.keys():\n",
    "            # Add the child node to the graph and create a directed edge from parent to child\n",
    "            graph.add_node(child)\n",
    "            if child not in inter_nodes:\n",
    "                graph.add_edge(parent, child)\n",
    "    return graph\n",
    "\n",
    "def check_impact_of_intervention_node(graph, inter_node, data, dict_nodes_thres, dict_edges_lag, n):\n",
    "    descendants_list = list(nx.descendants(graph, inter_node))\n",
    "    index_inter = list(data[data[inter_node] > dict_nodes_thres[inter_node][0]].index)\n",
    "    for node in descendants_list:\n",
    "        shortest_path = nx.shortest_path(graph, source=inter_node, target=node)\n",
    "        lag = 0\n",
    "        for i in range(len(shortest_path)-1):\n",
    "            lag+=dict_edges_lag[str((shortest_path[i],shortest_path[i+1]))]\n",
    "        index_node = [i+lag for i in index_inter if i+lag<n]\n",
    "        print(node, np.sum(data.loc[index_node][node] < dict_nodes_thres[node][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d06ab197-e124-4dd7-9510-c32f3df7ab45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# graphs_path: path for graphs\n",
    "# save_data_path: path to save generated data\n",
    "# save_info_path: path to save thresholds of nodes and lags of edges\n",
    "# n : number of sampling points\n",
    "# gamma_min: minimum lag\n",
    "# gamma_max: maximum lag\n",
    "def generate_historical_data_by_folder_SK(graphs_path, save_data_path, save_info_path, n, thres_min=0.7, thres_max=0.9, gamma_min=1, gamma_max=1, prob_inter=0.3, certain=True, epsilon=0.3):\n",
    "    # np.random.seed(seed=seed)\n",
    "    if not os.path.exists(save_data_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_data_path)\n",
    "    # check the existence of data information folder\n",
    "    if not os.path.exists(save_info_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_info_path)\n",
    "        \n",
    "    #################################################################\n",
    "    #################################################################\n",
    "    graph_files = [os.path.join(graphs_path, f) for f in os.listdir(graphs_path) if os.path.isfile(os.path.join(graphs_path, f))]\n",
    "\n",
    "    for json_file_path in tqdm(graph_files):\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            json_graph = json.load(json_file)\n",
    "\n",
    "        # Convert the loaded JSON data into a NetworkX graph\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=[])\n",
    "\n",
    "        nodes_list = list(graph.nodes())\n",
    "        edges_list = list(graph.edges())\n",
    "        topological_order = list(nx.topological_sort(graph))\n",
    "\n",
    "        data = {}\n",
    "        dict_nodes_thres = {}\n",
    "        dict_nodes_base = {}\n",
    "        coffes_edges = {}\n",
    "        children_list = []\n",
    "        dict_edges_lag = {}\n",
    "        \n",
    "        for node in nodes_list:\n",
    "            dict_nodes_base[node] = [np.round(np.random.uniform(low=0, high=0.1), 2)]\n",
    "            dict_nodes_thres[node] = [np.round(np.random.uniform(low=thres_min, high=thres_max), 2)]\n",
    "            \n",
    "            \n",
    "        for edge in edges_list:\n",
    "            dict_edges_lag[edge] = np.random.randint(low=gamma_min, high=gamma_max+1)\n",
    "            coffes_edges[edge] = dict_nodes_thres[edge[1]][0]\n",
    "            children_list.append(edge[1])\n",
    "\n",
    "        children_list = list(set(children_list))\n",
    "\n",
    "        for node in nodes_list:\n",
    "            if node not in children_list:\n",
    "                data[node] = []\n",
    "                for i in range(n):\n",
    "                    if np.random.uniform() < 1- prob_inter:\n",
    "                        data[node].append(dict_nodes_base[node][0])\n",
    "                    else: \n",
    "                        data[node].append(np.random.uniform(low=dict_nodes_thres[node][0], high=dict_nodes_thres[node][0]+1))\n",
    "            else:\n",
    "                data[node] = []\n",
    "\n",
    "        # propagation of interventions\n",
    "        for i in range(n):\n",
    "            for node in topological_order:\n",
    "                parents = list(graph.predecessors(node))\n",
    "                if len(parents) != 0:\n",
    "                    if np.random.uniform() < 1- prob_inter:\n",
    "                        value = dict_nodes_base[node][0]\n",
    "                        for par in parents:\n",
    "                            lag = dict_edges_lag[(par, node)]\n",
    "                            if i-lag < 0:\n",
    "                                value += 0\n",
    "                            elif data[par][i-lag] >= dict_nodes_thres[par][0]:\n",
    "                                value += coffes_edges[(par,node)]\n",
    "                        if not certain:\n",
    "                            if value >= dict_nodes_thres[node][0]:\n",
    "                                if np.random.uniform() < 1-epsilon:\n",
    "                                    value += 0\n",
    "                                else:\n",
    "                                    value = dict_nodes_base[node][0] \n",
    "                        data[node].append(value)\n",
    "                    else:\n",
    "                        data[node].append(np.random.uniform(low=dict_nodes_thres[node][0], high=dict_nodes_thres[node][0]+1))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "        \n",
    "        \n",
    "        edges_lag = {}\n",
    "        for key,value in dict_edges_lag.items():\n",
    "            edges_lag[str(key)] = value\n",
    "        \n",
    "        edges_coffe = {}\n",
    "        for key,value in coffes_edges.items():\n",
    "            edges_coffe[str(key)] = value\n",
    "\n",
    "        info = {'nodes_thres': dict_nodes_thres, 'nodes_base': dict_nodes_base, 'edges_lag': edges_lag, 'edges_coffe': edges_coffe}\n",
    "\n",
    "        data.to_csv(os.path.join(save_data_path, json_file_path.split('/')[1].replace('graph', 'data').replace('json', 'csv')), index=False)\n",
    "\n",
    "        save_data_info_path = os.path.join(save_info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        # Save the dictionary as a JSON file\n",
    "        with open(save_data_info_path, 'w') as json_file:\n",
    "            json.dump(info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0946ed5-6cfc-46d3-a1db-e23ea8f54dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graphs_path = 'graphs'\n",
    "save_data_path = os.path.join('certain_SK', 'historical_data')\n",
    "save_info_path = os.path.join('certain_SK', 'data_info')\n",
    "n = 2000\n",
    "thres_min=0.7 \n",
    "thres_max=0.9\n",
    "gamma_min = 1 \n",
    "gamma_max = 1 \n",
    "prob_inter = 0.3 \n",
    "epsilon = 0  \n",
    "certain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492157fc-5d4b-4975-a53e-92bbf46dc60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_historical_data_by_folder_SK(graphs_path=graphs_path, save_data_path=save_data_path, save_info_path=save_info_path, \n",
    "                                      n=n, thres_min=thres_min, thres_max=thres_max, gamma_min=gamma_min, gamma_max=gamma_max, \n",
    "                                      prob_inter=prob_inter, certain=certain, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4aa4ed1-e319-4eee-aa21-914d4e502385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_simulation_data_by_folder_SK(graphs_path, info_path, save_data_path, save_info_path, n, num_inters=1, certain=True, epsilon=0.3):\n",
    "    # np.random.seed(seed=seed)\n",
    "    if not os.path.exists(save_data_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_data_path)\n",
    "\n",
    "    if not os.path.exists(save_info_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_info_path)\n",
    "\n",
    "    graph_files = [os.path.join(graphs_path, f) for f in os.listdir(graphs_path) if os.path.isfile(os.path.join(graphs_path, f))]\n",
    "\n",
    "    for json_file_path in tqdm(graph_files):\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            json_graph = json.load(json_file)\n",
    "\n",
    "        data_info_path = os.path.join(info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        with open(data_info_path, 'r') as json_file:\n",
    "            data_info = json.load(json_file)\n",
    "\n",
    "        dict_nodes_thres = data_info['nodes_thres']\n",
    "        dict_nodes_base = data_info['nodes_base']\n",
    "        dict_edges_lag = data_info['edges_lag']\n",
    "        dict_edges_coffe = data_info['edges_coffe']\n",
    "        # Convert the loaded JSON data into a NetworkX graph\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=[])\n",
    "        if num_inters == 1:\n",
    "            inter_nodes = np.random.choice(graph.nodes, size=num_inters, replace=False)\n",
    "        else:\n",
    "            while True:\n",
    "                inter_nodes = np.random.choice(graph.nodes, size=num_inters, replace=False)\n",
    "                in_same_path = False\n",
    "                for node_1 in inter_nodes:\n",
    "                    for node_2 in inter_nodes:\n",
    "                        if node_1 != node_2:\n",
    "                            if node_1 in nx.ancestors(graph, node_2): # and node_2 not in nx.ancestors(graph, node_1)):\n",
    "                                in_same_path = True\n",
    "                if not in_same_path:\n",
    "                    break\n",
    "\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=inter_nodes)\n",
    "\n",
    "        nodes_list = list(graph.nodes())\n",
    "        edges_list = list(graph.edges())\n",
    "        topological_order = list(nx.topological_sort(graph))\n",
    "\n",
    "        data = {}\n",
    "        children_list = []\n",
    "\n",
    "        for edge in edges_list:\n",
    "            children_list.append(edge[1])\n",
    "\n",
    "        children_list = list(set(children_list))\n",
    "\n",
    "        for node in nodes_list:\n",
    "            if node in inter_nodes:\n",
    "                data[node] = np.random.uniform(low=dict_nodes_thres[node][0], high=dict_nodes_thres[node][0]+1, size=n)\n",
    "            elif node not in children_list and node not in inter_nodes:\n",
    "                data[node] = [dict_nodes_base[node][0] for i in range(n)]\n",
    "            else:\n",
    "                data[node] = []\n",
    "        \n",
    "        # transfer interventions\n",
    "        for i in range(n):\n",
    "            for node in topological_order:\n",
    "                parents = list(graph.predecessors(node))\n",
    "                if len(parents) != 0:\n",
    "                    value = dict_nodes_base[node][0]\n",
    "                    for par in parents:\n",
    "                        lag = dict_edges_lag[str((par, node))]\n",
    "                        if i-lag < 0:\n",
    "                            value += 0\n",
    "                        elif data[par][i-lag] >= dict_nodes_thres[par][0]:\n",
    "                            value += dict_edges_coffe[str((par,node))]\n",
    "                    if not certain:\n",
    "                        if value >= dict_nodes_thres[node][0]:\n",
    "                            if np.random.uniform() < 1-epsilon:\n",
    "                                value += 0\n",
    "                            else:\n",
    "                                value = dict_nodes_base[node][0] \n",
    "                    data[node].append(value)\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "        info = {'nodes_thres': dict_nodes_thres, 'nodes_base': dict_nodes_base, 'edges_lag': dict_edges_lag, 'edges_coffe': dict_edges_coffe, 'intervention_node': list(inter_nodes)}\n",
    "\n",
    "        data.to_csv(os.path.join(save_data_path, json_file_path.split('/')[1].replace('graph', 'data').replace('json', 'csv')), index=False)\n",
    "\n",
    "        data_info_path = os.path.join(save_info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        # Save the dictionary as a JSON file\n",
    "        with open(data_info_path, 'w') as json_file:\n",
    "            json.dump(info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787f92d9-1bca-4940-8430-ec80406b4101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 156.72it/s]\n"
     ]
    }
   ],
   "source": [
    "graphs_path = 'graphs'\n",
    "info_path = os.path.join('certain_SK', 'data_info')\n",
    "n = 500 # sampling points\n",
    "num_inters = 1\n",
    "certain = True\n",
    "epsilon = 0\n",
    "# save_data_path = os.path.join('certain_', 'data')\n",
    "# save_info_path = os.path.join('certain_', 'data_info')\n",
    "save_data_path = os.path.join('certain_SK', 'actual_data_'+str(num_inters)+'_inters')\n",
    "save_info_path = os.path.join('certain_SK', 'data_info_'+str(num_inters)+'_inters')\n",
    "generate_simulation_data_by_folder_SK(graphs_path=graphs_path, info_path=info_path, save_data_path=save_data_path, save_info_path=save_info_path, \n",
    "                                      n=n, num_inters=num_inters, certain=certain, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d487cb23-e496-42b5-a418-59ed44c8bdd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'b'): 0.7775816799346464,\n",
       " ('a', 'f'): 0.7407277949069354,\n",
       " ('b', 'c'): 0.1321493793871566,\n",
       " ('b', 'd'): 0.26484011777821814,\n",
       " ('c', 'd'): 0.3692049991008318,\n",
       " ('c', 'f'): 0.8241110587328954,\n",
       " ('d', 'e'): 0.8144996106075356,\n",
       " ('d', 'f'): 0.8304148711547787}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffes_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5b850d1-7b58-4fb9-83e8-30e0dc6e5b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# graphs_path: path for graphs\n",
    "# interventions_path: path of the intervention node\n",
    "# data_path: path to save generated data\n",
    "# info_path: path to save thresholds of nodes and lags of edges\n",
    "# n : number of sampling points\n",
    "# gamma_min: minimum lag\n",
    "# gamma_max: maximum lag\n",
    "# thres_min: minimum threshold for all nodes\n",
    "# thres_max: maximum threshold for all nodes\n",
    "def generate_historical_data_by_folder_PC(graphs_path, data_path, info_path, n, gamma_min,\n",
    "                             gamma_max, thres_min, thres_max, prob_inter, epsilon, seed=3344, self_loops=True, max_anomaly=10):\n",
    "    # np.random.seed(seed=seed)\n",
    "    self_lag = 1\n",
    "    if not os.path.exists(data_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(data_path)\n",
    "    # check the existence of data information folder\n",
    "    if not os.path.exists(info_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(info_path)\n",
    "\n",
    "    graph_files = [os.path.join(graphs_path, f) for f in os.listdir(graphs_path) if os.path.isfile(os.path.join(graphs_path, f))]\n",
    "\n",
    "    for json_file_path in tqdm(graph_files):\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            json_graph = json.load(json_file)\n",
    "\n",
    "        # Convert the loaded JSON data into a NetworkX graph\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=[])\n",
    "\n",
    "        nodes_list = list(graph.nodes())\n",
    "        edges_list = list(graph.edges())\n",
    "        topological_order = list(nx.topological_sort(graph))\n",
    "\n",
    "        data = {}\n",
    "        dict_nodes_thres = {}\n",
    "        children_list = []\n",
    "        dict_edges_lag = {}\n",
    "\n",
    "        for edge in edges_list:\n",
    "            dict_edges_lag[edge] = np.random.randint(low=gamma_min, high=gamma_max+1)\n",
    "            children_list.append(edge[1])\n",
    "\n",
    "        children_list = list(set(children_list))\n",
    "        for node in nodes_list:\n",
    "            dict_nodes_thres[node] = [np.round(np.random.uniform(low = thres_min, high = thres_max), 2)]\n",
    "\n",
    "        for node in nodes_list:\n",
    "            data[node] = []\n",
    "            if node not in children_list:\n",
    "                # data[node] = np.random.uniform(low=0, high=1, size=n)\n",
    "                for i in range(n):\n",
    "                    if self_loops and i-self_lag >= 0 and data[node][i-self_lag] >= dict_nodes_thres[node][0] and np.random.uniform() < 1-epsilon:\n",
    "                        data[node].append(np.random.uniform(low=dict_nodes_thres[node][0], high=1))\n",
    "                    else:         \n",
    "                        if np.random.uniform() < 1 - prob_inter:\n",
    "                            data[node].append(np.random.uniform(low=0, high=dict_nodes_thres[node][0]))\n",
    "                        else:\n",
    "                            data[node].append(np.random.uniform(low=dict_nodes_thres[node][0], high=1))\n",
    "                    \n",
    "                    if i-max_anomaly-1>=0 and np.sum(data[node][i-max_anomaly-1:i-1]>=dict_nodes_thres[node][0])==max_anomaly:\n",
    "                        data[node][i] = np.random.uniform(low=0, high=dict_nodes_thres[node][0])  \n",
    "            else:\n",
    "                data[node] = np.random.uniform(low=0, high=dict_nodes_thres[node][0], size=n)\n",
    "\n",
    "        # propagation of interventions\n",
    "        # for node in topological_order:\n",
    "        #     values = data[node]\n",
    "        #     children = list(graph.successors(node))\n",
    "        #     if len(children) != 0:\n",
    "        #         for i in range(len(values)):\n",
    "        #             if values[i] >= dict_nodes_thres[node]:\n",
    "        #                 for child in children:\n",
    "        #                     lag = dict_edges_lag[(node,child)]\n",
    "        #                     if i + lag < n and np.random.uniform() < 1-epsilon:\n",
    "        #                         data[child][i+lag] = np.random.uniform(low=dict_nodes_thres[child], high=1)\n",
    "        #     else:\n",
    "        #          continue\n",
    "\n",
    "        for node in topological_order:\n",
    "            parents = list(graph.predecessors(node))\n",
    "            if len(parents) != 0:\n",
    "                for i in range(len(data[node])):\n",
    "                    # abnormal_parent = False\n",
    "                    abnormal_parent = 0\n",
    "                    for par in parents:\n",
    "                        par_values = data[par]\n",
    "                        lag = dict_edges_lag[(par, node)]\n",
    "                        if i-lag >=0 and par_values[i-lag] >= dict_nodes_thres[par][0]:\n",
    "                            # abnormal_parent = True\n",
    "                            abnormal_parent += 1\n",
    "                    # if abnormal_parent and np.random.uniform() < 1-epsilon:\n",
    "                    #     data[node][i] = np.random.uniform(low=dict_nodes_thres[node][0], high=1)\n",
    "                    if self_loops and i-self_lag >=0 and data[node][i-self_lag] >= dict_nodes_thres[node][0]:\n",
    "                        abnormal_parent +=1\n",
    "                    if abnormal_parent != 0:\n",
    "                        for m in range(abnormal_parent):\n",
    "                            if np.random.uniform() < 1-epsilon:\n",
    "                                data[node][i] = np.random.uniform(low=dict_nodes_thres[node][0], high=1)\n",
    "                    else:\n",
    "                        if np.random.uniform() < prob_inter:\n",
    "                            data[node][i] = np.random.uniform(low=dict_nodes_thres[node][0], high=1)\n",
    "                            \n",
    "                    if i-max_anomaly-1>=0 and np.sum(data[node][i-max_anomaly-1:i-1]>=dict_nodes_thres[node][0])==max_anomaly:\n",
    "                        data[node][i] = np.random.uniform(low=0, high=dict_nodes_thres[node][0])\n",
    "            else:\n",
    "                 continue\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "        # ## *****************************************************************************************************\n",
    "        # ## Check data\n",
    "        # print(json_file_path.split('/')[1].split('.')[0])\n",
    "        # print(\"****Check impacts of the intervention node****\")\n",
    "        # print('Intervention node: ' + inter_node)\n",
    "        # print('Descendants of the intervention node:' + str(list(nx.descendants(graph, inter_node))))\n",
    "        # check_impact_of_intervention_node(graph=graph, inter_node=inter_node,\n",
    "        #                                   data=data, dict_nodes_thres=dict_nodes_thres, dict_edges_lag=dict_edges_lag, n=n)\n",
    "        # if not only_inter:\n",
    "        #     print(\"****Check impacts of the root****\")\n",
    "        #     root_nodes = [i for i in nodes_list if i not in children_list and i != inter_node]\n",
    "        #     for root in root_nodes:\n",
    "        #         print('root:' + root)\n",
    "        #         check_impact_of_intervention_node(graph=graph, inter_node=root,\n",
    "        #                                   data=data, dict_nodes_thres=dict_nodes_thres, dict_edges_lag=dict_edges_lag, n=n)\n",
    "        # ## *****************************************************************************************************\n",
    "        edges_lag = {}\n",
    "        for key,value in dict_edges_lag.items():\n",
    "            edges_lag[str(key)] = value\n",
    "\n",
    "        info = {'nodes_thres': dict_nodes_thres, 'edges_lag':edges_lag}\n",
    "        \n",
    "        last_node = topological_order[-1]\n",
    "        print(np.sum(data[last_node]>=dict_node_thres[last_node][0])/2000)\n",
    "        \n",
    "\n",
    "        data.to_csv(os.path.join(data_path, json_file_path.split('/')[1].replace('graph', 'data').replace('json', 'csv')), index=False)\n",
    "\n",
    "        data_info_path = os.path.join(info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        # Save the dictionary as a JSON file\n",
    "        with open(data_info_path, 'w') as json_file:\n",
    "            json.dump(info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39a8fbac-5c41-4b01-9673-f01e84d2fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "graphs_path = 'graphs'\n",
    "n = 2000 # sampling points\n",
    "gamma_min = 1\n",
    "gamma_max = 1\n",
    "thres_min = 0.7\n",
    "thres_max = 0.9\n",
    "epsilon = 0.5\n",
    "prob_inter = 0.1\n",
    "self_loops= False\n",
    "max_anomaly= 5\n",
    "# data_path = os.path.join('certain', 'historical_data')\n",
    "# info_path = os.path.join('certain', 'data_info')\n",
    "data_path = os.path.join('uncertain_' + str(epsilon), 'historical_data')\n",
    "info_path = os.path.join('uncertain_' + str(epsilon), 'data_info')\n",
    "data, dict_node_thres = generate_historical_data_by_folder_PC(graphs_path=graphs_path, data_path=data_path, info_path=info_path, n=n,\n",
    "                                   gamma_min=gamma_min, gamma_max=gamma_max, thres_min=thres_min,\n",
    "                                   thres_max=thres_max, prob_inter=prob_inter, epsilon=epsilon, self_loops=self_loops, max_anomaly=max_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3c7fe-1173-4b64-b899-198046d4b804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179555c-94f1-473d-a3aa-f47a8c629b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulation_data_by_folder_PC(graphs_path, info_path, save_data_path, save_info_path, n, epsilon, only_inter=False, num_inters=1, self_loops=False, max_anomaly=5, seed=3344):\n",
    "    self_lag = 1\n",
    "    # np.random.seed(seed=seed)\n",
    "    if not os.path.exists(save_data_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_data_path)\n",
    "\n",
    "    if not os.path.exists(save_info_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_info_path)\n",
    "\n",
    "    graph_files = [os.path.join(graphs_path, f) for f in os.listdir(graphs_path) if os.path.isfile(os.path.join(graphs_path, f))]\n",
    "\n",
    "    for json_file_path in graph_files:\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            json_graph = json.load(json_file)\n",
    "\n",
    "        data_info_path = os.path.join(info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        with open(data_info_path, 'r') as json_file:\n",
    "            data_info = json.load(json_file)\n",
    "\n",
    "        dict_nodes_thres = data_info['nodes_thres']\n",
    "        dict_edges_lag = data_info['edges_lag']\n",
    "        # Convert the loaded JSON data into a NetworkX graph\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=[])\n",
    "        if num_inters == 1:\n",
    "            inter_nodes = np.random.choice(graph.nodes, size=1, replace=False)\n",
    "        else:\n",
    "            while True:\n",
    "                inter_nodes = np.random.choice(graph.nodes, size=num_inters, replace=False)\n",
    "                in_same_path = False\n",
    "                for node_1 in inter_nodes:\n",
    "                    for node_2 in inter_nodes:\n",
    "                        if node_1 != node_2:\n",
    "                            if node_1 in nx.ancestors(graph, node_2): # and node_2 not in nx.ancestors(graph, node_1)):\n",
    "                                in_same_path = True\n",
    "                if not in_same_path:\n",
    "                    break\n",
    "\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=inter_nodes)\n",
    "\n",
    "        nodes_list = list(graph.nodes())\n",
    "        edges_list = list(graph.edges())\n",
    "        topological_order = list(nx.topological_sort(graph))\n",
    "\n",
    "        data = {}\n",
    "        children_list = []\n",
    "\n",
    "        for edge in edges_list:\n",
    "            children_list.append(edge[1])\n",
    "\n",
    "        children_list = list(set(children_list))\n",
    "\n",
    " \n",
    "        for node in nodes_list:\n",
    "            data[node] = []\n",
    "            if node in inter_nodes:\n",
    "                data[node] = np.random.uniform(low=dict_nodes_thres[node][0], high=1, size=n)\n",
    "                for i in range(n):\n",
    "                    if (i+1)%max_anomaly == 1:\n",
    "                        data[node][i] =  np.random.uniform(low=0, high=dict_nodes_thres[node][0])\n",
    "            else:\n",
    "                data[node] = np.random.uniform(low=0, high=dict_nodes_thres[node][0], size=n)\n",
    "\n",
    "\n",
    "        # transfer interventions\n",
    "        # for node in topological_order:\n",
    "        #     values = data[node]\n",
    "        #     children = list(graph.successors(node))\n",
    "        #     if len(children) != 0:\n",
    "        #         for i in range(len(values)):\n",
    "        #             if values[i] >= dict_nodes_thres[node][0]:\n",
    "        #                 for child in children:\n",
    "        #                     lag = dict_edges_lag[str((node,child))]\n",
    "        #                     if i + lag < n and np.random.uniform() < 1-epsilon:\n",
    "        #                         data[child][i+lag] = np.random.uniform(low=dict_nodes_thres[child][0], high=1)\n",
    "        #     else:\n",
    "        #          continue\n",
    "                    \n",
    "        for node in topological_order:\n",
    "            parents = list(graph.predecessors(node))\n",
    "            if len(parents) != 0:\n",
    "                for i in range(len(data[node])):\n",
    "                    # abnormal_parent = False\n",
    "                    abnormal_parent = 0\n",
    "                    for par in parents:\n",
    "                        par_values = data[par]\n",
    "                        lag = dict_edges_lag[(par, node)]\n",
    "                        if i-lag >=0 and par_values[i-lag] >= dict_nodes_thres[par][0]:\n",
    "                            # abnormal_parent = True\n",
    "                            abnormal_parent += 1\n",
    "                    # if abnormal_parent and np.random.uniform() < 1-epsilon:\n",
    "                    #     data[node][i] = np.random.uniform(low=dict_nodes_thres[node][0], high=1)\n",
    "                    if self_loops and i-self_lag >=0 and data[node][i-self_lag] >= dict_nodes_thres[node][0]:\n",
    "                        abnormal_parent +=1\n",
    "                    if abnormal_parent != 0:\n",
    "                        for m in range(abnormal_parent):\n",
    "                            if np.random.uniform() < 1-epsilon:\n",
    "                                data[node][i] = np.random.uniform(low=dict_nodes_thres[node][0], high=1)\n",
    "                            \n",
    "                    if i-max_anomaly-1>=0 and np.sum(data[node][i-max_anomaly-1:i-1]>=dict_nodes_thres[node][0])==max_anomaly:\n",
    "                        data[node][i] = np.random.uniform(low=0, high=dict_nodes_thres[node][0])\n",
    "            else:\n",
    "                 continue\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "        ## *****************************************************************************************************\n",
    "        ## Check data\n",
    "        print(json_file_path.split('/')[1].split('.')[0])\n",
    "        print(\"****Check impacts of the intervention node****\")\n",
    "        for node in inter_nodes:\n",
    "            print('Intervention node: ' + node)\n",
    "            print('Descendants of the intervention node:' + str(list(nx.descendants(graph, node))))\n",
    "            check_impact_of_intervention_node(graph=graph, inter_node=node,\n",
    "                                              data=data, dict_nodes_thres=dict_nodes_thres, dict_edges_lag=dict_edges_lag, n=n)\n",
    "        if not only_inter:\n",
    "            print(\"****Check impacts of the root****\")\n",
    "            root_nodes = [i for i in nodes_list if i not in children_list and i not in inter_nodes]\n",
    "            for root in root_nodes:\n",
    "                print('root:' + root)\n",
    "                check_impact_of_intervention_node(graph=graph, inter_node=root,\n",
    "                                          data=data, dict_nodes_thres=dict_nodes_thres, dict_edges_lag=dict_edges_lag, n=n)\n",
    "        ## *****************************************************************************************************\n",
    "\n",
    "        info = {'nodes_thres': dict_nodes_thres, 'edges_lag': dict_edges_lag, 'intervention_node': list(inter_nodes)}\n",
    "        \n",
    "        last_node = topological_order[-1]\n",
    "        print(np.sum(data[last_node]>=dict_nodes_thres[last_node][0])/n)\n",
    "\n",
    "        data.to_csv(os.path.join(save_data_path, json_file_path.split('/')[1].replace('graph', 'data').replace('json', 'csv')), index=False)\n",
    "\n",
    "        data_info_path = os.path.join(save_info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        # Save the dictionary as a JSON file\n",
    "        with open(data_info_path, 'w') as json_file:\n",
    "            json.dump(info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98800a86-e2da-4a99-8460-a2260779e627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f7946642-1cd6-4735-86ab-97f613f7d9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_historical_data_by_folder_SK(graphs_path, save_data_path, save_info_path, n, thres_min=0.7, thres_max=0.9, gamma_min=1, gamma_max=1, prob_inter=0.3, epsilon=0.3, self_loops=False, max_anomaly=5):\n",
    "    self_lag = 1\n",
    "    # np.random.seed(seed=seed)\n",
    "    if not os.path.exists(save_data_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_data_path)\n",
    "    # check the existence of data information folder\n",
    "    if not os.path.exists(save_info_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_info_path)\n",
    "\n",
    "    #################################################################\n",
    "    #################################################################\n",
    "    graph_files = [os.path.join(graphs_path, f) for f in os.listdir(graphs_path) if os.path.isfile(os.path.join(graphs_path, f))]\n",
    "\n",
    "    for json_file_path in tqdm(graph_files):\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            json_graph = json.load(json_file)\n",
    "\n",
    "        # Convert the loaded JSON data into a NetworkX graph\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=[])\n",
    "\n",
    "        nodes_list = list(graph.nodes())\n",
    "        edges_list = list(graph.edges())\n",
    "        topological_order = list(nx.topological_sort(graph))\n",
    "\n",
    "        data = {}\n",
    "        dict_nodes_thres = {}\n",
    "        dict_nodes_base = {}\n",
    "        coffes_edges = {}\n",
    "        children_list = []\n",
    "        dict_edges_lag = {}\n",
    "\n",
    "        for node in nodes_list:\n",
    "            dict_nodes_base[node] = [np.round(np.random.uniform(low=0, high=0.1), 2)]\n",
    "            dict_nodes_thres[node] = [np.round(np.random.uniform(low=thres_min, high=thres_max), 2)]\n",
    "\n",
    "\n",
    "        for edge in edges_list:\n",
    "            dict_edges_lag[edge] = np.random.randint(low=gamma_min, high=gamma_max+1)\n",
    "            coffes_edges[edge] = dict_nodes_thres[edge[1]][0]\n",
    "            children_list.append(edge[1])\n",
    "\n",
    "        children_list = list(set(children_list))\n",
    "        \n",
    "                \n",
    "        for node in nodes_list:\n",
    "            data[node] = []\n",
    "            if node not in children_list:\n",
    "                # data[node] = np.random.uniform(low=0, high=1, size=n)\n",
    "                for i in range(n):\n",
    "                    if self_loops and i-self_lag >= 0 and data[node][i-self_lag] >= dict_nodes_thres[node][0] and np.random.uniform() < 1-epsilon:\n",
    "                        data[node].append(dict_nodes_base[node][0]+dict_nodes_thres[node][0])\n",
    "                    else:         \n",
    "                        if np.random.uniform() < 1 - prob_inter:\n",
    "                            data[node].append(dict_nodes_base[node][0])\n",
    "                        else:\n",
    "                            data[node].append(np.random.uniform(low=dict_nodes_thres[node][0], high=dict_nodes_thres[node][0]+1))\n",
    "                    \n",
    "                    if i-max_anomaly-1>=0 and np.sum(data[node][i-max_anomaly-1:i-1]>=dict_nodes_thres[node][0])==max_anomaly:\n",
    "                        data[node][i] = dict_nodes_base[node][0]  \n",
    "            else:\n",
    "                data[node] = np.array([dict_nodes_base[node][0] for i in range(n)])\n",
    "\n",
    "        # propagation of interventions\n",
    "        for node in topological_order:\n",
    "            parents = list(graph.predecessors(node))\n",
    "            if len(parents) != 0:\n",
    "                for i in range(len(data[node])):\n",
    "                    # abnormal_parent = False\n",
    "                    abnormal_parent = 0\n",
    "                    for par in parents:\n",
    "                        par_values = data[par]\n",
    "                        lag = dict_edges_lag[(par, node)]\n",
    "                        if i-lag >=0 and par_values[i-lag] >= dict_nodes_thres[par][0]:\n",
    "                            # abnormal_parent = True\n",
    "                            abnormal_parent += 1\n",
    "                    # if abnormal_parent and np.random.uniform() < 1-epsilon:\n",
    "                    #     data[node][i] = np.random.uniform(low=dict_nodes_thres[node][0], high=1)\n",
    "                    if self_loops and i-self_lag >=0 and data[node][i-self_lag] >= dict_nodes_thres[node][0]:\n",
    "                        abnormal_parent +=1\n",
    "                    if abnormal_parent != 0:\n",
    "                        for m in range(abnormal_parent):\n",
    "                            if np.random.uniform() < 1-epsilon:\n",
    "                                data[node][i] += dict_nodes_thres[node][0]\n",
    "                    else:\n",
    "                        if np.random.uniform() < prob_inter:\n",
    "                            data[node][i] = np.random.uniform(low=dict_nodes_thres[node][0], high=dict_nodes_thres[node][0]+1)\n",
    "                            \n",
    "                    if i-max_anomaly-1>=0 and np.sum(data[node][i-max_anomaly-1:i-1]>=dict_nodes_thres[node][0])==max_anomaly:\n",
    "                        data[node][i] = dict_nodes_base[node][0]\n",
    "            else:\n",
    "                 continue\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "        edges_lag = {}\n",
    "        for key,value in dict_edges_lag.items():\n",
    "            edges_lag[str(key)] = value\n",
    "\n",
    "        edges_coffe = {}\n",
    "        for key,value in coffes_edges.items():\n",
    "            edges_coffe[str(key)] = value\n",
    "\n",
    "        info = {'nodes_thres': dict_nodes_thres, 'nodes_base': dict_nodes_base, 'edges_lag': edges_lag, 'edges_coffe': edges_coffe}\n",
    "        \n",
    "        last_node = topological_order[-1]\n",
    "        print(np.sum(data[last_node]>=dict_nodes_thres[last_node][0])/n)\n",
    "\n",
    "        data.to_csv(os.path.join(save_data_path, json_file_path.split('/')[1].replace('graph', 'data').replace('json', 'csv')), index=False)\n",
    "\n",
    "        save_data_info_path = os.path.join(save_info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        # Save the dictionary as a JSON file\n",
    "        with open(save_data_info_path, 'w') as json_file:\n",
    "            json.dump(info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba0eed69-089b-492d-b9b1-5cd5d665e06f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:00<00:04, 10.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519\n",
      "0.4225\n",
      "0.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:00<00:04, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3205\n",
      "0.3995\n",
      "0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:00<00:03, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.392\n",
      "0.4525\n",
      "0.4245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:01<00:03, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4065\n",
      "0.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:01<00:03, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3295\n",
      "0.4165\n",
      "0.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [00:01<00:03, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4825\n",
      "0.348\n",
      "0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:01<00:02, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n",
      "0.4865\n",
      "0.4955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:02<00:02, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.513\n",
      "0.407\n",
      "0.446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:02<00:02, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4275\n",
      "0.5805\n",
      "0.3455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:02<00:01, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3705\n",
      "0.479\n",
      "0.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:02<00:01, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.471\n",
      "0.3985\n",
      "0.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:03<00:01, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.496\n",
      "0.402\n",
      "0.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:03<00:01, 10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5035\n",
      "0.4755\n",
      "0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:03<00:00, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5025\n",
      "0.38\n",
      "0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [00:04<00:00, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5615\n",
      "0.552\n",
      "0.443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [00:04<00:00, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4255\n",
      "0.5645\n",
      "0.4195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.416\n",
      "0.502\n",
      "0.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate historical data\n",
    "graphs_path = 'graphs'\n",
    "n = 2000 # sampling points\n",
    "gamma_min = 1\n",
    "gamma_max = 1\n",
    "thres_min = 0.7\n",
    "thres_max = 0.9\n",
    "epsilon = 0.3\n",
    "prob_inter = 0.05\n",
    "self_loops = False\n",
    "max_anomaly = 5\n",
    "# data_path = os.path.join('certain_test', 'historical_data')\n",
    "# info_path = os.path.join('certain_test', 'data_info')\n",
    "data_path = os.path.join('SK_uncertain_test_' + str(epsilon), 'historical_data')\n",
    "info_path = os.path.join('SK_uncertain_test_' + str(epsilon), 'data_info')\n",
    "generate_historical_data_by_folder_SK(graphs_path=graphs_path, save_data_path=data_path, save_info_path=info_path, n=n,\n",
    "                                   gamma_min=gamma_min, gamma_max=gamma_max, thres_min=thres_min,\n",
    "                                   thres_max=thres_max, prob_inter=prob_inter, epsilon=epsilon,\n",
    "                                   self_loops=self_loops, max_anomaly=max_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "49e5b2b0-210c-49a1-8850-62a9f89eaac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_simulation_data_by_folder_SK(graphs_path, info_path, save_data_path, save_info_path, n, num_inters=1, epsilon=0.3, self_loops=False, max_anomaly=5):\n",
    "    self_lag = 1\n",
    "    # np.random.seed(seed=seed)\n",
    "    if not os.path.exists(save_data_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_data_path)\n",
    "\n",
    "    if not os.path.exists(save_info_path):\n",
    "        # If it doesn't exist, create the folder\n",
    "        os.makedirs(save_info_path)\n",
    "\n",
    "    graph_files = [os.path.join(graphs_path, f) for f in os.listdir(graphs_path) if os.path.isfile(os.path.join(graphs_path, f))]\n",
    "\n",
    "    for json_file_path in tqdm(graph_files):\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            json_graph = json.load(json_file)\n",
    "\n",
    "        data_info_path = os.path.join(info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        with open(data_info_path, 'r') as json_file:\n",
    "            data_info = json.load(json_file)\n",
    "\n",
    "        dict_nodes_thres = data_info['nodes_thres']\n",
    "        dict_nodes_base = data_info['nodes_base']\n",
    "        dict_edges_lag = data_info['edges_lag']\n",
    "        dict_edges_coffe = data_info['edges_coffe']\n",
    "        # Convert the loaded JSON data into a NetworkX graph\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=[])\n",
    "        if num_inters == 1:\n",
    "            inter_nodes = np.random.choice(graph.nodes, size=num_inters, replace=False)\n",
    "        else:\n",
    "            while True:\n",
    "                inter_nodes = np.random.choice(graph.nodes, size=num_inters, replace=False)\n",
    "                in_same_path = False\n",
    "                for node_1 in inter_nodes:\n",
    "                    for node_2 in inter_nodes:\n",
    "                        if node_1 != node_2:\n",
    "                            if node_1 in nx.ancestors(graph, node_2): # and node_2 not in nx.ancestors(graph, node_1)):\n",
    "                                in_same_path = True\n",
    "                if not in_same_path:\n",
    "                    break\n",
    "\n",
    "        graph = dict_to_graph(graph_dict=json_graph, inter_nodes=inter_nodes)\n",
    "\n",
    "        nodes_list = list(graph.nodes())\n",
    "        edges_list = list(graph.edges())\n",
    "        topological_order = list(nx.topological_sort(graph))\n",
    "\n",
    "        data = {}\n",
    "        children_list = []\n",
    "\n",
    "        for edge in edges_list:\n",
    "            children_list.append(edge[1])\n",
    "\n",
    "        children_list = list(set(children_list))\n",
    "        \n",
    "        for node in nodes_list:\n",
    "            data[node] = []\n",
    "            if node in inter_nodes:\n",
    "                data[node] = np.random.uniform(low=dict_nodes_thres[node][0], high=dict_nodes_thres[node][0]+1, size=n)\n",
    "                for i in range(n):\n",
    "                    if (i+1)%max_anomaly == 1:\n",
    "                        data[node][i] =  dict_nodes_base[node][0]\n",
    "            else:\n",
    "                data[node] = np.array([dict_nodes_base[node][0] for i in range(n)])\n",
    "\n",
    "        # transfer interventions\n",
    "        for node in topological_order:\n",
    "            parents = list(graph.predecessors(node))\n",
    "            if len(parents) != 0:\n",
    "                for i in range(len(data[node])):\n",
    "                    # abnormal_parent = False\n",
    "                    abnormal_parent = 0\n",
    "                    for par in parents:\n",
    "                        par_values = data[par]\n",
    "                        lag = dict_edges_lag[str((par, node))]\n",
    "                        if i-lag >=0 and par_values[i-lag] >= dict_nodes_thres[par][0]:\n",
    "                            # abnormal_parent = True\n",
    "                            abnormal_parent += 1\n",
    "                    # if abnormal_parent and np.random.uniform() < 1-epsilon:\n",
    "                    #     data[node][i] = np.random.uniform(low=dict_nodes_thres[node][0], high=1)\n",
    "                    if self_loops and i-self_lag >=0 and data[node][i-self_lag] >= dict_nodes_thres[node][0]:\n",
    "                        abnormal_parent +=1\n",
    "                    if abnormal_parent != 0:\n",
    "                        for m in range(abnormal_parent):\n",
    "                            if np.random.uniform() < 1-epsilon:\n",
    "                                data[node][i] += dict_nodes_thres[node][0]\n",
    "                    if i-max_anomaly-1>=0 and np.sum(data[node][i-max_anomaly-1:i-1]>=dict_nodes_thres[node][0])==max_anomaly:\n",
    "                        data[node][i] = dict_nodes_base[node][0]\n",
    "            else:\n",
    "                 continue\n",
    "\n",
    "        data = pd.DataFrame(data)\n",
    "        \n",
    "        only_inter = True\n",
    "        ## *****************************************************************************************************\n",
    "        ## Check data\n",
    "        print(json_file_path.split('/')[1].split('.')[0])\n",
    "        print(\"****Check impacts of the intervention node****\")\n",
    "        for node in inter_nodes:\n",
    "            print('Intervention node: ' + node)\n",
    "            print('Descendants of the intervention node:' + str(list(nx.descendants(graph, node))))\n",
    "            check_impact_of_intervention_node(graph=graph, inter_node=node,\n",
    "                                              data=data, dict_nodes_thres=dict_nodes_thres, dict_edges_lag=dict_edges_lag, n=n)\n",
    "        if not only_inter:\n",
    "            print(\"****Check impacts of the root****\")\n",
    "            root_nodes = [i for i in nodes_list if i not in children_list and i not in inter_nodes]\n",
    "            for root in root_nodes:\n",
    "                print('root:' + root)\n",
    "                check_impact_of_intervention_node(graph=graph, inter_node=root,\n",
    "                                          data=data, dict_nodes_thres=dict_nodes_thres, dict_edges_lag=dict_edges_lag, n=n)\n",
    "        ## *****************************************************************************************************\n",
    "\n",
    "        info = {'nodes_thres': dict_nodes_thres, 'nodes_base': dict_nodes_base, 'edges_lag': dict_edges_lag, 'edges_coffe': dict_edges_coffe, 'intervention_node': list(inter_nodes)}\n",
    "\n",
    "        data.to_csv(os.path.join(save_data_path, json_file_path.split('/')[1].replace('graph', 'data').replace('json', 'csv')), index=False)\n",
    "\n",
    "        data_info_path = os.path.join(save_info_path, json_file_path.split('/')[1].replace('graph', 'info'))\n",
    "        # Save the dictionary as a JSON file\n",
    "        with open(data_info_path, 'w') as json_file:\n",
    "            json.dump(info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "21365995-e951-451c-a8b1-cd53d5f0ef38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 279.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_3_2_6\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:['e', 'd', 'f']\n",
      "e 0\n",
      "d 0\n",
      "f 10\n",
      "graph_2_3_6\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['e', 'f']\n",
      "e 0\n",
      "f 0\n",
      "graph_2_2_5\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_1_2_8\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: a\n",
      "Descendants of the intervention node:['d', 'b', 'e', 'c', 'f']\n",
      "d 0\n",
      "b 0\n",
      "e 0\n",
      "c 0\n",
      "f 0\n",
      "graph_1_2_5\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['c', 'e', 'd', 'f']\n",
      "c 0\n",
      "e 0\n",
      "d 0\n",
      "f 0\n",
      "graph_2_3_1\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: e\n",
      "Descendants of the intervention node:['f']\n",
      "f 0\n",
      "graph_1_2_9\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_1_2_2\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:['e', 'd']\n",
      "e 0\n",
      "d 0\n",
      "graph_2_2_9\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_2_3_0\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:[]\n",
      "graph_1_3_9\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['c', 'e', 'd']\n",
      "c 0\n",
      "e 0\n",
      "d 0\n",
      "graph_1_3_1\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:[]\n",
      "graph_1_2_0\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['d']\n",
      "d 0\n",
      "graph_2_3_5\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:['e', 'f']\n",
      "e 0\n",
      "f 0\n",
      "graph_2_2_3\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:['e', 'd', 'f']\n",
      "e 0\n",
      "d 0\n",
      "f 10\n",
      "graph_1_3_8\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_3_2_9\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_1_3_0\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: a\n",
      "Descendants of the intervention node:['d', 'b', 'e', 'c', 'f']\n",
      "d 0\n",
      "b 0\n",
      "e 0\n",
      "c 0\n",
      "f 0\n",
      "graph_2_2_2\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:['f']\n",
      "f 0\n",
      "graph_3_2_8\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_2_2_1\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: a\n",
      "Descendants of the intervention node:['d', 'b', 'e', 'c', 'f']\n",
      "d 0\n",
      "b 0\n",
      "e 0\n",
      "c 0\n",
      "f 0\n",
      "graph_1_3_4\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: a\n",
      "Descendants of the intervention node:['d', 'b', 'e', 'c', 'f']\n",
      "d 0\n",
      "b 0\n",
      "e 0\n",
      "c 0\n",
      "f 0\n",
      "graph_1_2_3\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['c', 'd', 'f']\n",
      "c 0\n",
      "d 0\n",
      "f 0\n",
      "graph_2_2_8\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:['e']\n",
      "e 0\n",
      "graph_3_2_4\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:['e', 'd', 'f']\n",
      "e 0\n",
      "d 0\n",
      "f 0\n",
      "graph_1_3_6\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:[]\n",
      "graph_1_3_3\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:[]\n",
      "graph_2_2_6\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['d', 'f']\n",
      "d 0\n",
      "f 0\n",
      "graph_2_3_4\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_2_3_9\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: e\n",
      "Descendants of the intervention node:['f']\n",
      "f 0\n",
      "graph_1_3_7\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:['e']\n",
      "e 0\n",
      "graph_2_3_7\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:[]\n",
      "graph_3_2_2\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: a\n",
      "Descendants of the intervention node:['d', 'b', 'e', 'c', 'f']\n",
      "d 0\n",
      "b 0\n",
      "e 9\n",
      "c 0\n",
      "f 9\n",
      "graph_1_3_5\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:[]\n",
      "graph_2_2_4\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: a\n",
      "Descendants of the intervention node:['d', 'b', 'e', 'c', 'f']\n",
      "d 0\n",
      "b 0\n",
      "e 10\n",
      "c 0\n",
      "f 8\n",
      "graph_2_2_7\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_2_2_0\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:['e', 'f']\n",
      "e 0\n",
      "f 0\n",
      "graph_1_2_6\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: a\n",
      "Descendants of the intervention node:['d', 'b', 'e', 'c', 'f']\n",
      "d 0\n",
      "b 0\n",
      "e 0\n",
      "c 0\n",
      "f 0\n",
      "graph_3_2_1\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_1_2_7\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:[]\n",
      "graph_2_3_8\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['e', 'd', 'f']\n",
      "e 0\n",
      "d 0\n",
      "f 0\n",
      "graph_3_2_0\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:['e', 'f']\n",
      "e 0\n",
      "f 10\n",
      "graph_3_2_3\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:['f']\n",
      "f 0\n",
      "graph_1_2_4\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['c', 'e', 'd', 'f']\n",
      "c 0\n",
      "e 0\n",
      "d 0\n",
      "f 0\n",
      "graph_2_3_3\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_3_2_7\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: a\n",
      "Descendants of the intervention node:['d', 'b', 'e', 'c', 'f']\n",
      "d 0\n",
      "b 0\n",
      "e 0\n",
      "c 0\n",
      "f 10\n",
      "graph_2_3_2\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: c\n",
      "Descendants of the intervention node:['d', 'f']\n",
      "d 0\n",
      "f 0\n",
      "graph_1_3_2\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: d\n",
      "Descendants of the intervention node:['f']\n",
      "f 0\n",
      "graph_3_2_5\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: f\n",
      "Descendants of the intervention node:[]\n",
      "graph_1_2_1\n",
      "****Check impacts of the intervention node****\n",
      "Intervention node: b\n",
      "Descendants of the intervention node:['c', 'e', 'd', 'f']\n",
      "c 0\n",
      "e 0\n",
      "d 0\n",
      "f 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graphs_path = 'graphs'\n",
    "info_path = os.path.join('SK_uncertain_test_0.3', 'data_info')\n",
    "n = 50 # sampling points\n",
    "num_inters = 1\n",
    "epsilon = 0\n",
    "self_loops = False\n",
    "max_anomaly = 5\n",
    "# save_data_path = os.path.join('certain_', 'data')\n",
    "#save_info_path = os.path.join('certain_', 'data_info')\n",
    "save_data_path = os.path.join('SK_uncertain_test_0.3', 'actual_data_'+str(num_inters)+'_inters')\n",
    "save_info_path = os.path.join('SK_uncertain_test_0.3', 'data_info_'+str(num_inters)+'_inters')\n",
    "generate_simulation_data_by_folder_SK(graphs_path=graphs_path, info_path=info_path, save_data_path=save_data_path, save_info_path=save_info_path,\n",
    "                                      n=n, num_inters=num_inters, epsilon=epsilon, self_loops=self_loops, max_anomaly=max_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288843d-cf61-4e23-bbe7-549dfff8711a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
