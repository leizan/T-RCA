{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f1f9c7-2cc4-497c-976e-d5a3d3480aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b1574e-455f-4506-b4ce-57858d62be9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_mechanisme = ['different_path', 'one_path']\n",
    "list_process = ['E1'] #['E1', 'E2']\n",
    "list_scenarios = ['certain', 'certain_SL', 'uncertain_0.3', 'uncertain_0.3_SL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf8fda8-4e5b-4f88-bd96-4be431edfdad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:35<00:00,  4.31s/it]\n",
      "100%|██████████| 50/50 [03:58<00:00,  4.76s/it]\n",
      "100%|██████████| 50/50 [03:29<00:00,  4.19s/it]\n",
      "100%|██████████| 50/50 [03:48<00:00,  4.56s/it]\n",
      "100%|██████████| 50/50 [03:44<00:00,  4.49s/it]\n",
      "100%|██████████| 50/50 [03:54<00:00,  4.68s/it]\n",
      "100%|██████████| 50/50 [03:37<00:00,  4.34s/it]\n",
      "100%|██████████| 50/50 [04:07<00:00,  4.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# graph_path = 'graphs'\n",
    "gamma_max= 1\n",
    "for mechanisme in list_mechanisme:\n",
    "    for process in list_process:\n",
    "        for scenario in list_scenarios:\n",
    "            if mechanisme == 'different_path':\n",
    "                actual_data_path = os.path.join(process, scenario, 'actual_data_2_inters_2000')\n",
    "            elif mechanisme == 'one_path':\n",
    "                actual_data_path = os.path.join(process, scenario, 'actual_data_same_path_2_inters_2000')\n",
    "                \n",
    "            actual_data_files = [os.path.join(actual_data_path, f) for f in os.listdir(actual_data_path) if os.path.isfile(os.path.join(actual_data_path, f))]\n",
    "            \n",
    "            for actual_data_file in tqdm(actual_data_files):\n",
    "                if mechanisme == 'different_path':\n",
    "                    historical_data = pd.read_csv(actual_data_file.replace('actual_data_2_inters_2000', 'historical_data_20000')) \n",
    "                elif mechanisme == 'one_path':\n",
    "                    historical_data = pd.read_csv(actual_data_file.replace('actual_data_same_path_2_inters_2000', 'historical_data_20000'))\n",
    "                actual_data = pd.read_csv(actual_data_file)\n",
    "                actual_data = pd.concat([historical_data, actual_data], ignore_index=True)\n",
    "                data_info = os.path.join(os.path.join(process, scenario), 'data_info_20000', actual_data_file.split('/')[-1].replace('data', 'info').replace('csv', 'json'))\n",
    "                with open(data_info, 'r') as json_file:\n",
    "                    data_info = json.load(json_file)\n",
    "                threshold_of_data = data_info['nodes_thres']\n",
    "                length_of_data = actual_data.values.shape[0]\n",
    "                list_variables = actual_data.columns\n",
    "                window_size = gamma_max*(len(list_variables)-1)\n",
    "                log_data = {}\n",
    "                log_data['case:concept:name'] = []\n",
    "                log_data['observation'] = []\n",
    "                log_data['time:timestamp'] = []\n",
    "                case_index = 1\n",
    "                for index in range(length_of_data):\n",
    "                    for i in range(window_size+1):\n",
    "                        if index + i < length_of_data:\n",
    "                            for var in list_variables:\n",
    "                                if actual_data[var][index+i] >= threshold_of_data[var]:\n",
    "                                    log_data['case:concept:name'].append(case_index)\n",
    "                                    log_data['observation'].append(var)\n",
    "                                    log_data['time:timestamp'].append(index+i)\n",
    "                    case_index+=1\n",
    "                                              \n",
    "                log_data = pd.DataFrame(log_data)\n",
    "                log_data = log_data.sort_values(by='time:timestamp')\n",
    "\n",
    "                if mechanisme == 'different_path':\n",
    "                    output_data_path = os.path.join(process, scenario, 'LOG_actual_data_2_inters_2000')\n",
    "                    if not os.path.exists(output_data_path):\n",
    "                    # If it doesn't exist, create the folder\n",
    "                        os.makedirs(output_data_path)  \n",
    "                elif mechanisme == 'one_path':\n",
    "                    output_data_path = os.path.join(process, scenario, 'LOG_actual_data_same_path_2_inters_2000')\n",
    "                    if not os.path.exists(output_data_path):\n",
    "                    # If it doesn't exist, create the folder\n",
    "                        os.makedirs(output_data_path) \n",
    "                log_data.to_csv(os.path.join(output_data_path, actual_data_file.split('/')[-1]), index=False)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e9374-f2b8-4475-9abc-bb81021bc739",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normal data generating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d364034-cb6f-4d38-8fb0-a8b542928fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_mechanisme = ['EasyRCA']\n",
    "list_scenarios = ['Parametric_2'] # ['Parametric', 'Structual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf1f2ca-b4a8-4368-a38c-d1c839379b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:02<00:00,  6.05s/it]\n"
     ]
    }
   ],
   "source": [
    "historical_data_length = 20000\n",
    "gamma_max= 1\n",
    "normal_ratio = 0.9\n",
    "for mechanisme in list_mechanisme:\n",
    "    for scenario in list_scenarios:\n",
    "        whole_data_path = os.path.join(mechanisme, scenario, 'data')\n",
    "        \n",
    "        whole_data_files = [os.path.join(whole_data_path, f) for f in os.listdir(whole_data_path) if os.path.isfile(os.path.join(whole_data_path, f))]\n",
    "\n",
    "        for whole_data_file in tqdm(whole_data_files):\n",
    "            whole_data = pd.read_csv(whole_data_file)\n",
    "            param_data = whole_data.head(historical_data_length)\n",
    "            threshold_of_data = {}\n",
    "            for node in whole_data.columns:\n",
    "                threshold_of_data[node] = np.sort(param_data[node])[int(normal_ratio*param_data[node].shape[0])]\n",
    "            length_of_data = whole_data.values.shape[0]\n",
    "            list_variables = whole_data.columns\n",
    "            window_size = gamma_max*(len(list_variables)-1)\n",
    "            log_data = {}\n",
    "            log_data['case:concept:name'] = []\n",
    "            log_data['observation'] = []\n",
    "            log_data['time:timestamp'] = []\n",
    "            case_index = 1\n",
    "            for index in range(length_of_data):\n",
    "                for i in range(window_size+1):\n",
    "                    if index + i < length_of_data:\n",
    "                        for var in list_variables:\n",
    "                            if whole_data[var][index+i] >= threshold_of_data[var]:\n",
    "                                log_data['case:concept:name'].append(case_index)\n",
    "                                log_data['observation'].append(var)\n",
    "                                log_data['time:timestamp'].append(index+i)\n",
    "                case_index+=1\n",
    "\n",
    "            log_data = pd.DataFrame(log_data)\n",
    "            log_data = log_data.sort_values(by='time:timestamp')\n",
    "\n",
    "            output_data_path = os.path.join(mechanisme, scenario, 'LOG')\n",
    "            if not os.path.exists(output_data_path):\n",
    "            # If it doesn't exist, create the folder\n",
    "                os.makedirs(output_data_path)  \n",
    "           \n",
    "            log_data.to_csv(os.path.join(output_data_path, whole_data_file.split('/')[-1]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d0ec3-ccc2-40f9-801f-662fb11abdc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dowhy data generating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4408f870-e355-477d-b2b9-a0589d381087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please pay attention to the name of the folder\n",
    "list_mechanisme = ['Dowhy_0.5']\n",
    "list_scenarios = ['different_path', 'one_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c078629-be93-47f1-9620-724ddc503710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [15:58<00:00, 19.16s/it]\n",
      "100%|██████████| 50/50 [18:34<00:00, 22.30s/it]\n"
     ]
    }
   ],
   "source": [
    "historical_data_length = 20000\n",
    "gamma_max= 1\n",
    "normal_ratio = 0.9\n",
    "for mechanisme in list_mechanisme:\n",
    "    for scenario in list_scenarios:\n",
    "        whole_data_path = os.path.join(mechanisme, scenario, 'data')\n",
    "        \n",
    "        whole_data_files = [os.path.join(whole_data_path, f) for f in os.listdir(whole_data_path) if os.path.isfile(os.path.join(whole_data_path, f))]\n",
    "\n",
    "        for whole_data_file in tqdm(whole_data_files):\n",
    "            whole_data = pd.read_csv(whole_data_file)\n",
    "            param_data = whole_data.head(historical_data_length)\n",
    "            threshold_of_data = {}\n",
    "            for node in whole_data.columns:\n",
    "                threshold_of_data[node] = np.sort(param_data[node])[int(normal_ratio*param_data[node].shape[0])]\n",
    "            length_of_data = whole_data.values.shape[0]\n",
    "            list_variables = whole_data.columns\n",
    "            window_size = gamma_max*(len(list_variables)-1)\n",
    "            log_data = {}\n",
    "            log_data['case:concept:name'] = []\n",
    "            log_data['observation'] = []\n",
    "            log_data['time:timestamp'] = []\n",
    "            case_index = 1\n",
    "            for index in range(length_of_data):\n",
    "                for i in range(window_size+1):\n",
    "                    if index + i < length_of_data:\n",
    "                        for var in list_variables:\n",
    "                            if whole_data[var][index+i] >= threshold_of_data[var]:\n",
    "                                log_data['case:concept:name'].append(case_index)\n",
    "                                log_data['observation'].append(var)\n",
    "                                log_data['time:timestamp'].append(index+i)\n",
    "                case_index+=1\n",
    "\n",
    "            log_data = pd.DataFrame(log_data)\n",
    "            log_data = log_data.sort_values(by='time:timestamp')\n",
    "\n",
    "            output_data_path = os.path.join(mechanisme, scenario, 'LOG')\n",
    "            if not os.path.exists(output_data_path):\n",
    "            # If it doesn't exist, create the folder\n",
    "                os.makedirs(output_data_path)  \n",
    "           \n",
    "            log_data.to_csv(os.path.join(output_data_path, whole_data_file.split('/')[-1]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bf756-ec7e-41f7-a4a4-f2fb84019752",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Monitoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4bbd619-a658-4339-9b0c-ffe64cb44456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "simplify_node_name = {\n",
    "    'Real time merger bolt de not_found sur Storm-1': 'Real time merger bolt',\n",
    "    'Check message bolt de not_found sur storm-1': 'Check message bolt',\n",
    "    'Message dispatcher bolt de not_found sur storm-1': 'Message dispatcher bolt',\n",
    "    'Metric bolt de not_found sur Storm-1': 'Metric bolt',\n",
    "    'Pre-Message dispatcher bolt de not_found sur storm-1': 'Pre-Message dispatcher bolt',\n",
    "    'capacity_last_metric_bolt de Apache-Storm-bolt_capacity_topology - monitoring_ingestion sur prd-ovh-storm-01': 'Last_metric_bolt',\n",
    "    'capacity_elastic_search_bolt de Apache-Storm-bolt_capacity_topology - monitoring_ingestion sur prd-ovh-storm-01': 'Elastic_search_bolt',\n",
    "    'Group status information bolt de not_found sur storm-1': 'Group status information bolt'\n",
    "}\n",
    "\n",
    "column_name_transfer = {'capacity_last_metric_bolt': 'Last_metric_bolt',\n",
    "                        'capacity_elastic_search_bolt': 'Elastic_search_bolt',\n",
    "                        'pre_Message_dispatcher_bolt': 'Pre-Message dispatcher bolt',\n",
    "                        'check_message_bolt': 'Check message bolt',\n",
    "                        'message_dispatcher_bolt': 'Message dispatcher bolt',\n",
    "                        'metric_bolt': 'Metric bolt',\n",
    "                        'group_status_information_bolt': 'Group status information bolt',\n",
    "                        'Real_time_merger_bolt': 'Real time merger bolt'\n",
    "}\n",
    "\n",
    "true_root_causes = ['Elastic_search_bolt', 'Pre-Message dispatcher bolt']\n",
    "\n",
    "boolean_variables = []\n",
    "whole_data = pd.DataFrame()\n",
    "dict_anomaly = pd.DataFrame()\n",
    "directoryPath = '../real_monitoring_data/'\n",
    "\n",
    "for file_name in glob.glob(directoryPath + '*.csv'):\n",
    "    if \"data_with_incident_between_46683_and_46783\" not in file_name:\n",
    "        col_value = pd.read_csv(file_name, low_memory=False)\n",
    "        with open(file_name.replace('.csv', '.json')) as json_file:\n",
    "            x_descri = json.load(json_file)\n",
    "        whole_data[simplify_node_name[x_descri[\"metric_name\"]]] = col_value['value']\n",
    "        dict_anomaly[simplify_node_name[x_descri[\"metric_name\"]]] = x_descri[\"anomalies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8066b164-3b97-486f-ac2e-f5ce455a1f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomaly_start = 46683\n",
    "anomaly_end = 46783\n",
    "normal_ratio = 0.9\n",
    "\n",
    "param_data = whole_data.head(anomaly_start)\n",
    "\n",
    "threshold_of_data = {}\n",
    "for node in param_data.columns:\n",
    "    threshold_of_data[node] = np.sort(param_data[node])[int(normal_ratio*param_data[node].shape[0])]\n",
    "length_of_data = whole_data.values.shape[0]\n",
    "list_variables = whole_data.columns\n",
    "window_size = gamma_max*(len(list_variables)-1)\n",
    "log_data = {}\n",
    "log_data['case:concept:name'] = []\n",
    "log_data['observation'] = []\n",
    "log_data['time:timestamp'] = []\n",
    "case_index = 1\n",
    "for index in range(length_of_data):\n",
    "    for i in range(window_size+1):\n",
    "        if index + i < length_of_data:\n",
    "            for var in list_variables:\n",
    "                if whole_data[var][index+i] >= threshold_of_data[var]:\n",
    "                    log_data['case:concept:name'].append(case_index)\n",
    "                    log_data['observation'].append(var)\n",
    "                    log_data['time:timestamp'].append(index+i)\n",
    "    case_index+=1\n",
    "\n",
    "log_data = pd.DataFrame(log_data)\n",
    "log_data = log_data.sort_values(by='time:timestamp')\n",
    " \n",
    "log_data.to_csv('../monitring_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ab6cc8-bc39-4f1e-919f-2304e29fb328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>observation</th>\n",
       "      <th>time:timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652633</th>\n",
       "      <td>70585</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>70587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652638</th>\n",
       "      <td>70587</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>70587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652624</th>\n",
       "      <td>70583</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>70587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652636</th>\n",
       "      <td>70586</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>70587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652639</th>\n",
       "      <td>70588</td>\n",
       "      <td>Real time merger bolt</td>\n",
       "      <td>70587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>652640 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        case:concept:name            observation  time:timestamp\n",
       "0                       4  Real time merger bolt              10\n",
       "21                     10  Real time merger bolt              10\n",
       "15                      9  Real time merger bolt              10\n",
       "10                      8  Real time merger bolt              10\n",
       "6                       7  Real time merger bolt              10\n",
       "...                   ...                    ...             ...\n",
       "652633              70585  Real time merger bolt           70587\n",
       "652638              70587  Real time merger bolt           70587\n",
       "652624              70583  Real time merger bolt           70587\n",
       "652636              70586  Real time merger bolt           70587\n",
       "652639              70588  Real time merger bolt           70587\n",
       "\n",
       "[652640 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35a845-edd1-41c4-84bb-ef0e38270257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50670ddb-551a-42f0-8b49-a1efeed0b84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3a23c-690b-42d7-8e0e-63b1ccf5eb64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4aa031-d109-45b5-ae84-d41bb3faf4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_mechanisme = ['different_path'] # ['different_path', 'one_path']\n",
    "list_process = ['E1'] #['E1', 'E2']\n",
    "list_scenarios = ['certain'] # ['certain', 'certain_SL', 'uncertain_0.3', 'uncertain_0.3_SL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac9034b2-e15b-4e3c-a825-6371dc83b0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other causes\n",
      "['b', 'f']\n",
      "other causes\n",
      "['a', 'f']\n",
      "other causes\n",
      "['a', 'b']\n",
      "other causes\n",
      "['c', 'f']\n",
      "other causes\n",
      "['a', 'f']\n",
      "other causes\n",
      "['a', 'c']\n",
      "other causes\n",
      "['c', 'b']\n",
      "other causes\n",
      "['a', 'b']\n",
      "other causes\n",
      "['a', 'c']\n",
      "other causes\n",
      "['b']\n",
      "other causes\n",
      "['a']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "sys.path.append('/home/lzan/Bureau/Dynamic causal graph/root-cause-analysis/RAITIA')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from baseline.AITIA.Inference import Inference\n",
    "\n",
    "list_variables = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "\n",
    "\n",
    "for mechanisme in list_mechanisme:\n",
    "    for process in list_process:\n",
    "        for scenario in list_scenarios:\n",
    "            if mechanisme == 'different_path':\n",
    "                actual_data_path = os.path.join(process, scenario, 'LOG_actual_data_2_inters_2000')\n",
    "            elif mechanisme == 'one_path':\n",
    "                actual_data_path = os.path.join(process, scenario, 'LOG_actual_data_same_path_2_inters_2000')\n",
    "                \n",
    "\n",
    "set_log_files = [os.path.join(actual_data_path, f) for f in os.listdir(actual_data_path) if os.path.isfile(os.path.join(actual_data_path, f))]\n",
    "\n",
    "for log_file in set_log_files[1:2]:\n",
    "    inference = Inference(log_file, pb = False)\n",
    "    inference.generate_hypotheses_for_effects(causes = inference.alphabet, effects = inference.alphabet)\n",
    "    inference.test_for_prima_facie()\n",
    "    all_epsilon_averages = inference.calculate_average_epsilons()\n",
    "    print(all_epsilon_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ad77577-2ef9-4c01-bf19-6a5167206efb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'c'): 0.41081934610908344,\n",
       " ('b', 'c'): 0.5230329868658357,\n",
       " ('f', 'c'): 0.6998298969598098,\n",
       " ('a', 'b'): 0.6046461519225521,\n",
       " ('c', 'b'): 0.6169633626298179,\n",
       " ('f', 'b'): 0.6141794203613968,\n",
       " ('a', 'f'): 0.41682059810890904,\n",
       " ('c', 'f'): 0.6895196111808372,\n",
       " ('b', 'f'): 0.5399877626544682,\n",
       " ('a', 'd'): -0.06406307830806346,\n",
       " ('b', 'd'): 0.7565368676206115}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_epsilon_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "248d4dd9-9f65-4a87-8e3c-b33d6abff784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "if len(all_epsilon_averages.keys())==0:\n",
    "    pref_root_cause = []\n",
    "else:\n",
    "    list_effect = []\n",
    "    pred_root_cause = []\n",
    "\n",
    "\n",
    "    for edge in all_epsilon_averages.keys():\n",
    "        list_effect.append(edge[1])\n",
    "\n",
    "    list_effect = list(set(list_effect))\n",
    "\n",
    "    for effect in list_effect:\n",
    "        list_edges = []\n",
    "        list_epsilons = []\n",
    "        for edge in all_epsilon_averages.keys():\n",
    "            if edge[1] == effect:\n",
    "                list_edges.append(edge)\n",
    "                list_epsilons.append(all_epsilon_averages[edge])\n",
    "\n",
    "        list_epsilons = np.array(list_epsilons)\n",
    "        if np.std(list_epsilons) == 0:\n",
    "            z_scores = (list_epsilons-np.mean(list_epsilons))\n",
    "        else:\n",
    "            z_scores = (list_epsilons-np.mean(list_epsilons))/np.std(list_epsilons)\n",
    "        p_values = stats.norm.sf(abs(z_scores))*2\n",
    "\n",
    "        res, p_values_adapt = fdrcorrection(p_values, alpha=alpha)\n",
    "        for i in np.where(res==True)[0]:\n",
    "            pred_root_cause.append(list_edges[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94ea92ea-b4a4-4925-b826-faa84a09b190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'f']\n"
     ]
    }
   ],
   "source": [
    "pred_root_cause = list(set(pred_root_cause))\n",
    "print(pred_root_cause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b37f8d-c097-4cde-aa9e-e6b71ec81a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b40dc4b-48a5-4f16-88c5-b38ca32a110c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rpy2.robjects import numpy2ri, r, FloatVector\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "\n",
    "# Convert numpy arrays to R objects\n",
    "numpy2ri.activate()\n",
    "\n",
    "# Install and load the fdrtool package in R\n",
    "# r('install.packages(\"fdrtool\")')\n",
    "fdrtool = importr('fdrtool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a979bdc4-259e-487e-b513-a3faae085992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_epsilon_averages = {('a_t_1>0.8', 'a_t'): -0.006353078676086981, ('b_t_1>0.81', 'a_t'): -0.010453027046809268, ('d_t_1>0.77', 'a_t'): -0.0002159425228454498, ('c_t_1>0.75', 'a_t'): 0.0016278590484477967, ('e_t_1>0.78', 'a_t'): -0.0031319646113668496, ('f_t_1>0.88', 'a_t'): -0.00436052863915595, ('a_t_1>0.8', 'b_t'): 0.9035687319786583, ('b_t_1>0.81', 'b_t'): -0.009449299849309672, ('d_t_1>0.77', 'b_t'): -0.003827818653238041, ('c_t_1>0.75', 'b_t'): -0.012339257600468001, ('e_t_1>0.78', 'b_t'): 0.003685227805564789, ('f_t_1>0.88', 'b_t'): 0.011209264333918296, ('a_t_1>0.8', 'd_t'): 0.9038343715711326, ('b_t_1>0.81', 'd_t'): -0.010179292095904468, ('d_t_1>0.77', 'd_t'): -0.009286167270716938, ('c_t_1>0.75', 'd_t'): -0.014733574730934462, ('e_t_1>0.78', 'd_t'): -0.005826854997717101, ('f_t_1>0.88', 'd_t'): 0.002212161556768499, ('a_t_1>0.8', 'c_t'): -0.001847436841506489, ('b_t_1>0.81', 'c_t'): 0.8985670333521618, ('d_t_1>0.77', 'c_t'): 0.34772616465863726, ('c_t_1>0.75', 'c_t'): -0.004775653258291346, ('e_t_1>0.78', 'c_t'): -0.005506604862541309, ('f_t_1>0.88', 'c_t'): -0.002307756650838366, ('a_t_1>0.8', 'e_t'): 0.006348340122962437, ('b_t_1>0.81', 'e_t'): -0.0029780998005594084, ('d_t_1>0.77', 'e_t'): 0.002250651184571473, ('c_t_1>0.75', 'e_t'): 0.8933357071631354, ('e_t_1>0.78', 'e_t'): -0.0021874331346052656, ('f_t_1>0.88', 'e_t'): -0.005919179851838652, ('a_t_1>0.8', 'f_t'): -0.010802722868314796, ('b_t_1>0.81', 'f_t'): -0.0175974070188879, ('d_t_1>0.77', 'f_t'): -0.0035348304456794266, ('c_t_1>0.75', 'f_t'): 0.8946535481297984, ('e_t_1>0.78', 'f_t'): -0.007463291774489245, ('f_t_1>0.88', 'f_t'): -0.004502987831500072}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4202a3b-240b-4b9f-85ae-ea8c1284785b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_effect = []\n",
    "pred_root_cause = []\n",
    "\n",
    "for edge in all_epsilon_averages.keys():\n",
    "    list_effect.append(edge[1])\n",
    "    \n",
    "list_effect = list(set(list_effect))\n",
    "\n",
    "causes = []\n",
    "for effect in list_effect:\n",
    "    list_edges = []\n",
    "    list_epsilons = []\n",
    "    for edge in all_epsilon_averages.keys():\n",
    "        if edge[1] == effect:\n",
    "            list_edges.append(edge)\n",
    "            list_epsilons.append(all_epsilon_averages[edge])\n",
    "    \n",
    "    list_epsilons = np.array(list_epsilons)\n",
    "    z_scores = (list_epsilons-np.mean(list_epsilons))/np.std(list_epsilons)\n",
    "    p_values = stats.norm.sf(abs(z_scores))*2\n",
    "\n",
    "    res, p_values_adapt = fdrcorrection(p_values, alpha=alpha)\n",
    "    for i in np.where(res==True)[0]:\n",
    "        pred_root_cause.append(list_edges[i][0].split('_')[0])\n",
    "pred_root_cause = list(set(pred_root_cause))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b80d53dd-f640-4200-a508-d2ac651c78d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_root_cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "516b97ab-1ae4-4795-9165-79d0f6bbb7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a_t_1>0.8', 'c_t'),\n",
       " ('b_t_1>0.81', 'c_t'),\n",
       " ('d_t_1>0.77', 'c_t'),\n",
       " ('c_t_1>0.75', 'c_t'),\n",
       " ('e_t_1>0.78', 'c_t'),\n",
       " ('f_t_1>0.88', 'c_t')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaaaf66-afec-4996-a03f-75b08d6dda19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
